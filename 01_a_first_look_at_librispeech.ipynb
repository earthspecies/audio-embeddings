{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by downloading the librispeech dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-23 13:30:47--  http://www.openslr.org/resources/12/dev-clean.tar.gz\n",
      "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
      "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 337926286 (322M) [application/x-gzip]\n",
      "Saving to: ‘data/dev-clean.tar.gz’\n",
      "\n",
      "dev-clean.tar.gz    100%[===================>] 322.27M  7.10MB/s    in 45s     \n",
      "\n",
      "2020-09-23 13:31:32 (7.13 MB/s) - ‘data/dev-clean.tar.gz’ saved [337926286/337926286]\n",
      "\n",
      "--2020-09-23 13:31:32--  http://www.openslr.org/resources/12/dev-other.tar.gz\n",
      "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
      "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 314305928 (300M) [application/x-gzip]\n",
      "Saving to: ‘data/dev-other.tar.gz’\n",
      "\n",
      "dev-other.tar.gz    100%[===================>] 299.75M  8.20MB/s    in 52s     \n",
      "\n",
      "2020-09-23 13:32:24 (5.78 MB/s) - ‘data/dev-other.tar.gz’ saved [314305928/314305928]\n",
      "\n",
      "--2020-09-23 13:32:25--  http://www.openslr.org/resources/12/test-clean.tar.gz\n",
      "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
      "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 346663984 (331M) [application/x-gzip]\n",
      "Saving to: ‘data/test-clean.tar.gz’\n",
      "\n",
      "test-clean.tar.gz   100%[===================>] 330.60M  7.58MB/s    in 63s     \n",
      "\n",
      "2020-09-23 13:33:28 (5.24 MB/s) - ‘data/test-clean.tar.gz’ saved [346663984/346663984]\n",
      "\n",
      "--2020-09-23 13:33:28--  http://www.openslr.org/resources/12/test-other.tar.gz\n",
      "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
      "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 328757843 (314M) [application/x-gzip]\n",
      "Saving to: ‘data/test-other.tar.gz’\n",
      "\n",
      "test-other.tar.gz   100%[===================>] 313.53M  4.34MB/s    in 84s     \n",
      "\n",
      "2020-09-23 13:34:53 (3.74 MB/s) - ‘data/test-other.tar.gz’ saved [328757843/328757843]\n",
      "\n",
      "--2020-09-23 13:34:53--  http://www.openslr.org/resources/12/train-clean-100.tar.gz\n",
      "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
      "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6387309499 (5.9G) [application/x-gzip]\n",
      "Saving to: ‘data/train-clean-100.tar.gz’\n",
      "\n",
      "train-clean-100.tar 100%[===================>]   5.95G  11.0MB/s    in 11m 19s \n",
      "\n",
      "2020-09-23 13:46:12 (8.98 MB/s) - ‘data/train-clean-100.tar.gz’ saved [6387309499/6387309499]\n",
      "\n",
      "--2020-09-23 13:46:12--  http://www.openslr.org/resources/12/train-clean-360.tar.gz\n",
      "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
      "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23049477885 (21G) [application/x-gzip]\n",
      "Saving to: ‘data/train-clean-360.tar.gz’\n",
      "\n",
      "train-clean-360.tar 100%[===================>]  21.47G  8.93MB/s    in 52m 18s \n",
      "\n",
      "2020-09-23 14:38:31 (7.00 MB/s) - ‘data/train-clean-360.tar.gz’ saved [23049477885/23049477885]\n",
      "\n",
      "--2020-09-23 14:38:34--  http://www.openslr.org/resources/12/train-other-500.tar.gz\n",
      "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
      "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30593501606 (28G) [application/x-gzip]\n",
      "Saving to: ‘data/train-other-500.tar.gz’\n",
      "\n",
      "train-other-500.tar 100%[===================>]  28.49G  13.0MB/s    in 41m 34s \n",
      "\n",
      "2020-09-23 15:20:09 (11.7 MB/s) - ‘data/train-other-500.tar.gz’ saved [30593501606/30593501606]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fnames = [\n",
    "    'dev-clean.tar.gz',\n",
    "    'dev-other.tar.gz',\n",
    "    'test-clean.tar.gz',\n",
    "    'test-other.tar.gz',\n",
    "    'train-clean-100.tar.gz',\n",
    "    'train-clean-360.tar.gz',\n",
    "    'train-other-500.tar.gz',    \n",
    "]\n",
    "    \n",
    "for fn in fnames:\n",
    "    !wget http://www.openslr.org/resources/12/{fn} -P data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "for fn in fnames:\n",
    "    !tar -xvf data/{fn} -C data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio recordings were aligned with text using the [Montreal Forced Aligner](https://montreal-forced-aligner.readthedocs.io/). We will train our embeddings in an unsupervised way (meaning the text labels corresponding to audio will not be used) however we still want to have the ability to chunk the audio into meaningful parts (we want a complete utterance to constitute an exampole).\n",
    "\n",
    "Let's obtain labels from this very helpful [repository](https://github.com/CorentinJ/librispeech-alignments). Unfortunately since they are stored on google drive you will need to download them manually on your workstation and upload to the `data` directory in the root of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip data/LibriSpeech-Alignments.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "from IPython.lib.display import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data, we can adapt the parser from this repository from which we obtained the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "librispeech_root = \"data/LibriSpeech\"\n",
    "\n",
    "def split_audio(audio_fpath, words, end_times):\n",
    "    # Load the audio waveform\n",
    "    sample_rate = 16000     # Sampling rate of LibriSpeech \n",
    "    wav, _ = librosa.load(audio_fpath, sample_rate)\n",
    "    \n",
    "    start_times = np.array([0.0] + end_times[:-1])\n",
    "    end_times = np.array(end_times)\n",
    "    assert len(words) == len(end_times) == len(start_times)\n",
    "    assert words[0] == '' and words[-1] == ''\n",
    "        \n",
    "    segments = []\n",
    "    for word, st, et in zip(words, start_times, end_times):\n",
    "        if word == '': continue\n",
    "        utterance = wav[int(st*sample_rate):int(et*sample_rate)]\n",
    "        segments.append([word, utterance])\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate train examples and store them on disk. This way our code for training the model will be more streamlined than if we wanted to do something fancy with indexing in a wave file. Simpler code - smaller chance of introducing a bug!\n",
    "\n",
    "Plus this gives us a chance to offload some computation to before training, which is always nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract MFCC features, we will use functionality from the [python_speech_features](https://github.com/jameslyons/python_speech_features) repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_speech_features.base import mfcc\n",
    "import pandas as pd\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_audio(audio):\n",
    "    fn = f'data/examples/{uuid.uuid4().hex}.pkl'\n",
    "    pd.to_pickle(mfcc(audio), fn)\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11h 26min 34s, sys: 15min 56s, total: 11h 42min 30s\n",
      "Wall time: 3h 13min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "source_words, target_words, source_fns, target_fns, set_names, speaker_ids, book_ids = [], [], [], [], [], [], []\n",
    "\n",
    "window_size = 3\n",
    "offsets = list(range(-window_size+1, window_size))\n",
    "offsets.remove(0)\n",
    "\n",
    "for set_name in os.listdir(librispeech_root):\n",
    "    if set_name not in ['train-clean-360', 'train-clean-100', 'test-clean', 'dev-clean']: continue\n",
    "    set_dir = os.path.join(librispeech_root, set_name)\n",
    "    if not os.path.isdir(set_dir):\n",
    "        continue\n",
    "    for speaker_id in os.listdir(set_dir):\n",
    "        speaker_dir = os.path.join(set_dir, speaker_id)\n",
    "        for book_id in os.listdir(speaker_dir):\n",
    "            book_dir = os.path.join(speaker_dir, book_id)\n",
    "            alignment_fpath = os.path.join(book_dir, \"%s-%s.alignment.txt\" % \n",
    "                                           (speaker_id, book_id))\n",
    "            \n",
    "            if not os.path.exists(alignment_fpath):\n",
    "                raise Exception(\"Alignment file not found. Did you download and merge the txt \"\n",
    "                                \"alignments with your LibriSpeech dataset?\")\n",
    "\n",
    "            with open(alignment_fpath, \"r\") as alignment_file:\n",
    "                for line in alignment_file:\n",
    "\n",
    "                    # Retrieve the utterance id, the words as a list and the end_times as a list\n",
    "                    utterance_id, words, end_times = line.strip().split(' ')\n",
    "                    words = words.replace('\\\"', '').split(',')\n",
    "                    end_times = [float(e) for e in end_times.replace('\\\"', '').split(',')]\n",
    "                    audio_fpath = os.path.join(book_dir, utterance_id + '.flac')\n",
    "                    \n",
    "                    segments = split_audio(audio_fpath, words, end_times)\n",
    "                    segments_processed = [[word, store_audio(audio)] for word, audio in segments]\n",
    "        \n",
    "                    for i, (word, path) in enumerate(segments_processed):\n",
    "                        for offset in offsets:\n",
    "                            if i + offset < 0 or i + offset > len(segments_processed)-1: continue\n",
    "\n",
    "                            source_words.append(word), target_words.append(segments_processed[i+offset][0]),\n",
    "                            source_fns.append(path), target_fns.append(segments_processed[i+offset][1])\n",
    "                            set_names.append(set_name), speaker_ids.append(speaker_id), book_ids.append(book_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\n",
    "    'source_word': source_words,\n",
    "    'target_word': target_words,\n",
    "    'source_fn': source_fns,\n",
    "    'target_fn': target_fns,\n",
    "    'set_name': set_names,\n",
    "    'speaker_id': speaker_ids,\n",
    "    'book_id': book_ids\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17937758, 7)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_word</th>\n",
       "      <th>target_word</th>\n",
       "      <th>source_fn</th>\n",
       "      <th>target_fn</th>\n",
       "      <th>set_name</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>book_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>FELT</td>\n",
       "      <td>data/examples/53dfbe1cffca4994b848b6b117acb365...</td>\n",
       "      <td>data/examples/f95d4d56b23f4c06920f666bb4fa65cf...</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>THAT</td>\n",
       "      <td>data/examples/53dfbe1cffca4994b848b6b117acb365...</td>\n",
       "      <td>data/examples/caaa01a2dc014e4b9c4a0e832a471d97...</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FELT</td>\n",
       "      <td>I</td>\n",
       "      <td>data/examples/f95d4d56b23f4c06920f666bb4fa65cf...</td>\n",
       "      <td>data/examples/53dfbe1cffca4994b848b6b117acb365...</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FELT</td>\n",
       "      <td>THAT</td>\n",
       "      <td>data/examples/f95d4d56b23f4c06920f666bb4fa65cf...</td>\n",
       "      <td>data/examples/caaa01a2dc014e4b9c4a0e832a471d97...</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FELT</td>\n",
       "      <td>IT</td>\n",
       "      <td>data/examples/f95d4d56b23f4c06920f666bb4fa65cf...</td>\n",
       "      <td>data/examples/b3332ce1a360418eb9f836c38f9ff2aa...</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_word target_word                                          source_fn  \\\n",
       "0           I        FELT  data/examples/53dfbe1cffca4994b848b6b117acb365...   \n",
       "1           I        THAT  data/examples/53dfbe1cffca4994b848b6b117acb365...   \n",
       "2        FELT           I  data/examples/f95d4d56b23f4c06920f666bb4fa65cf...   \n",
       "3        FELT        THAT  data/examples/f95d4d56b23f4c06920f666bb4fa65cf...   \n",
       "4        FELT          IT  data/examples/f95d4d56b23f4c06920f666bb4fa65cf...   \n",
       "\n",
       "                                           target_fn         set_name  \\\n",
       "0  data/examples/f95d4d56b23f4c06920f666bb4fa65cf...  train-clean-360   \n",
       "1  data/examples/caaa01a2dc014e4b9c4a0e832a471d97...  train-clean-360   \n",
       "2  data/examples/53dfbe1cffca4994b848b6b117acb365...  train-clean-360   \n",
       "3  data/examples/caaa01a2dc014e4b9c4a0e832a471d97...  train-clean-360   \n",
       "4  data/examples/b3332ce1a360418eb9f836c38f9ff2aa...  train-clean-360   \n",
       "\n",
       "  speaker_id book_id  \n",
       "0       7000   83696  \n",
       "1       7000   83696  \n",
       "2       7000   83696  \n",
       "3       7000   83696  \n",
       "4       7000   83696  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['train-clean-360', 'train-clean-100', 'test-clean', 'dev-clean'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/examples.csv', header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
