{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.learner import *\n",
    "from fastai.data.all import *\n",
    "from fastai.callback.tracker import SaveModelCallback\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib2 import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30 s, sys: 3.09 s, total: 33.1 s\n",
      "Wall time: 33.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17937758, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv('data/examples_with_length_speech2vec_vocab.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.6 s, sys: 10.7 s, total: 27.3 s\n",
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fn2features = pd.read_pickle('data/fn2feature.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(df[df.in_speech2vec_vocab].source_word.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35932"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_word</th>\n",
       "      <th>target_word</th>\n",
       "      <th>source_fn</th>\n",
       "      <th>target_fn</th>\n",
       "      <th>set_name</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>distance_from_target</th>\n",
       "      <th>audio_fpath</th>\n",
       "      <th>source_length</th>\n",
       "      <th>target_length</th>\n",
       "      <th>in_speech2vec_vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>FELT</td>\n",
       "      <td>8af4aebcf4a74004b02db9f88d99e89a</td>\n",
       "      <td>1cb9442ec1a6468282da309756e2ff57</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>1</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>THAT</td>\n",
       "      <td>8af4aebcf4a74004b02db9f88d99e89a</td>\n",
       "      <td>2f60546c930c47068ee0a129e6d51c39</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>2</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FELT</td>\n",
       "      <td>I</td>\n",
       "      <td>1cb9442ec1a6468282da309756e2ff57</td>\n",
       "      <td>8af4aebcf4a74004b02db9f88d99e89a</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>1</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FELT</td>\n",
       "      <td>THAT</td>\n",
       "      <td>1cb9442ec1a6468282da309756e2ff57</td>\n",
       "      <td>2f60546c930c47068ee0a129e6d51c39</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>1</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FELT</td>\n",
       "      <td>IT</td>\n",
       "      <td>1cb9442ec1a6468282da309756e2ff57</td>\n",
       "      <td>155ad336d88c4cbf814a1237983b5b18</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>2</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_word target_word                         source_fn  \\\n",
       "0           I        FELT  8af4aebcf4a74004b02db9f88d99e89a   \n",
       "1           I        THAT  8af4aebcf4a74004b02db9f88d99e89a   \n",
       "2        FELT           I  1cb9442ec1a6468282da309756e2ff57   \n",
       "3        FELT        THAT  1cb9442ec1a6468282da309756e2ff57   \n",
       "4        FELT          IT  1cb9442ec1a6468282da309756e2ff57   \n",
       "\n",
       "                          target_fn         set_name  speaker_id  book_id  \\\n",
       "0  1cb9442ec1a6468282da309756e2ff57  train-clean-360        7000    83696   \n",
       "1  2f60546c930c47068ee0a129e6d51c39  train-clean-360        7000    83696   \n",
       "2  8af4aebcf4a74004b02db9f88d99e89a  train-clean-360        7000    83696   \n",
       "3  2f60546c930c47068ee0a129e6d51c39  train-clean-360        7000    83696   \n",
       "4  155ad336d88c4cbf814a1237983b5b18  train-clean-360        7000    83696   \n",
       "\n",
       "   distance_from_target  \\\n",
       "0                     1   \n",
       "1                     2   \n",
       "2                     1   \n",
       "3                     1   \n",
       "4                     2   \n",
       "\n",
       "                                                        audio_fpath  \\\n",
       "0  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac   \n",
       "1  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac   \n",
       "2  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac   \n",
       "3  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac   \n",
       "4  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac   \n",
       "\n",
       "   source_length  target_length  in_speech2vec_vocab  \n",
       "0             16             22                 True  \n",
       "1             16             12                 True  \n",
       "2             22             16                 True  \n",
       "3             22             12                 True  \n",
       "4             22              8                 True  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.37 s, sys: 200 ms, total: 5.57 s\n",
      "Wall time: 5.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "unique_fns = df.source_fn.unique()\n",
    "np.random.shuffle(unique_fns)\n",
    "lengths = []\n",
    "for i, features in enumerate(fn2features.values()):\n",
    "    lengths.append(features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.028019713968394"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f78c19f6c50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(features.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mean = -5\n",
    "dataset_std = 15\n",
    "\n",
    "def normalize_data(ary):\n",
    "    return (ary - dataset_mean) / dataset_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_list(): return list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# word2row_idxs = defaultdict(empty_list)\n",
    "\n",
    "# for idx, row in df.iterrows():\n",
    "#     word2row_idxs[row.source_word].append(idx)\n",
    "    \n",
    "# pd.to_pickle(word2row_idxs, 'data/word2row_idxs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2row_idxs = pd.read_pickle('data/word2row_idxs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(fn, pad_to=max(lengths), pad_left=False):\n",
    "    ary = fn2features[fn][:pad_to]\n",
    "    example = np.zeros((pad_to, 13))\n",
    "    if pad_left:\n",
    "        example[-ary.shape[0]:, :] = ary\n",
    "    else: example[:ary.shape[0], :] = ary\n",
    "    return example.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.nan in vocab: vocab.remove(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, n):\n",
    "        self.vocab = vocab * n\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    def __getitem__(self, idx):\n",
    "        row_idx = np.random.randint(len(word2row_idxs[self.vocab[idx]]))\n",
    "        source_fn = df.source_fn[row_idx]\n",
    "        target_fn = df.target_fn[row_idx]\n",
    "        x = normalize_data(prepare_features(source_fn, pad_left=True))\n",
    "        y = normalize_data(prepare_features(target_fn))\n",
    "        return np.stack((x, y)), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 2048\n",
    "LR = 1e-3\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "train_dl = DataLoader(Dataset(270), BS, NUM_WORKERS, shuffle=True)\n",
    "valid_dl = DataLoader(Dataset(30), BS, NUM_WORKERS)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got the following error while training:\n",
    "\n",
    "# DataLoader worker (pid 2073) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.\n",
    "# trying the solution I found here: https://github.com/pytorch/pytorch/issues/5040\n",
    "# which is to execute\n",
    "!sudo umount /dev/shm/ && sudo mount -t tmpfs -o rw,nosuid,nodev,noexec,relatime,size=50G shm /dev/shm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidirectional encoder, 1 layer, concatenate hidden state\n",
    "class Model(Module):\n",
    "    def __init__(self, hidden_size=25, num_layers_encoder=3):\n",
    "        self.return_embeddings = False\n",
    "        self.num_layers_encoder = num_layers_encoder\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.encoder= nn.LSTM(\n",
    "            input_size=13,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=self.num_layers_encoder,\n",
    "            batch_first=True,\n",
    "            dropout=0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=2*hidden_size+13,\n",
    "            hidden_size=2*hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.lin = nn.Linear(2*hidden_size, 13)\n",
    "            \n",
    "    def forward(self, source_and_target_features):\n",
    "        source_features = source_and_target_features[:, 0]\n",
    "        target_features = source_and_target_features[:, 1]\n",
    "        _, (embeddings, _) = self.encoder(source_features)\n",
    "    \n",
    "        embeddings = torch.cat((embeddings[-1], embeddings[-2]), 1)\n",
    "        if self.return_embeddings: return embeddings\n",
    "        \n",
    "        target_features = torch.cat((torch.zeros(target_features.shape[0], 1, 13).cuda(), target_features), 1)\n",
    "        inputs = torch.cat(\n",
    "            (\n",
    "                target_features[:, :-1, :],\n",
    "                embeddings.unsqueeze(1).repeat(1, target_features.shape[1]-1, 1)\n",
    "            ), 2)\n",
    "        x, _ = self.decoder(inputs, (embeddings.unsqueeze(0), torch.zeros_like(embeddings.unsqueeze(0))))\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(\n",
    "    dls.cuda(),\n",
    "    Model().cuda(),\n",
    "    loss_func=MSELoss(),\n",
    "    lr=[1e-5, 1e-3, 1e-3],\n",
    "    opt_func=Adam,\n",
    "    splitter=lambda model: L(model.encoder, model.decoder, model.lin).map(params)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/encoder_weights.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-25c2c44c91ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/encoder_weights.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_state_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/encoder_weights.pth'"
     ]
    }
   ],
   "source": [
    "encoder_state_dict = torch.load('models/encoder_weights.pth')\n",
    "learn.model.load_state_dict(encoder_state_dict['model'], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(10, cbs=SaveModelCallback(fname='1e-3_Adam_tf_pretrained', every_epoch=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner at 0x7fd01b117450>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('1e-3_Adam_tf_pretrained_9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate embedding for each unique word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_utterances = df[df.set_name.isin(['train-clean-360', 'train-clean-100', 'dev-clean'])].drop_duplicates(['source_fn'])\n",
    "df_unique_utterances.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetAllUtterances():\n",
    "    def __len__(self):\n",
    "        return df_unique_utterances.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        source_fn = df_unique_utterances.iloc[idx].source_fn\n",
    "        target_fn = df_unique_utterances.iloc[idx].target_fn\n",
    "        x = normalize_data(prepare_features(source_fn, pad_left=True))\n",
    "        y = normalize_data(prepare_features(target_fn))\n",
    "        return np.stack((x, y)), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dl = DataLoader(DatasetAllUtterances(), BS, NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "learn.model.return_embeddings = True\n",
    "learn.model.train = False\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "with torch.no_grad():    \n",
    "    for batch in all_dl:\n",
    "        embeddings = learn.model(batch[0].cuda())\n",
    "        all_embeddings.append(embeddings.detach().cpu().squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = torch.cat(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.return_embeddings = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(all_dl):\n",
    "        outputs = learn.model(batch[0].cuda())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(outputs[31].cpu().numpy().T[:, :20])\n",
    "axs[1].imshow(batch[1][31].cpu().numpy().T[:, :20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(outputs[0].cpu().numpy().T[:, :20])\n",
    "axs[1].imshow(batch[1][0].cpu().numpy().T[:, :20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(outputs[30].cpu().numpy().T[:, :20])\n",
    "axs[1].imshow(batch[1][30].cpu().numpy().T[:, :20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(outputs[15].cpu().numpy().T[:, :20])\n",
    "axs[1].imshow(batch[1][15].cpu().numpy().T[:, :20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# word2row_idxs_unique_utterances = defaultdict(empty_list)\n",
    "\n",
    "# for idx, row in df_unique_utterances.iterrows():\n",
    "#     word2row_idxs_unique_utterances[row.source_word].append(idx)\n",
    "    \n",
    "# pd.to_pickle(word2row_idxs_unique_utterances, 'word2row_idxs_unique_utterances.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2row_idxs_unique_utterances = pd.read_pickle('word2row_idxs_unique_utterances.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embedding = {}\n",
    "\n",
    "for k, v in word2row_idxs_unique_utterances.items():\n",
    "    word2embedding[k] = all_embeddings[np.array(v)].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embedding_without_nans= {}\n",
    "nans_encountered = 0\n",
    "for k, v in word2embedding.items():\n",
    "    if k in vocab and k == k and (not np.isnan(v.numpy()).any()):\n",
    "        word2embedding_without_nans[k] = v.numpy()\n",
    "    else: nans_encountered += 1\n",
    "\n",
    "print(f'Encountered rows with nan values: {nans_encountered}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Embeddings(\n",
    "    np.array(list(word2embedding_without_nans.values())),\n",
    "    [w.lower() for w in list(word2embedding_without_nans.keys())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in ['fast', 'lost', 'small', 'true', 'crazy', 'slow']:\n",
    "    print(f'{w}: {e.nn_words_to(e[w])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating embeddings using [word-embeddings-benchmarks](https://github.com/kudkudak/word-embeddings-benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from six import iteritems\n",
    "from web.datasets.similarity import fetch_MEN, fetch_WS353, fetch_SimLex999\n",
    "from web.embeddings import fetch_GloVe\n",
    "from web.evaluate import evaluate_similarity\n",
    "from web.embedding import Embedding, Vocabulary\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {\n",
    "    \"MEN\": fetch_MEN(),\n",
    "    \"WS353\": fetch_WS353(),\n",
    "    \"SIMLEX999\": fetch_SimLex999()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_embeddings = Embedding(\n",
    "    Vocabulary([w.lower() for w in list(word2embedding_without_nans.keys())]),\n",
    "    np.array(list(word2embedding_without_nans.values()))\n",
    ")\n",
    "\n",
    "speech2vec = KeyedVectors.load_word2vec_format('../speech2vec-pretrained-vectors/speech2vec/50.vec', binary=False) \n",
    "speech2vec_embeddings = Embedding(Vocabulary(list(speech2vec.vocab.keys())), speech2vec.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in iteritems(tasks):\n",
    "    print(\"Spearman correlation of scores on {} {}\".format(name, evaluate_similarity(our_embeddings, data.X, data.y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in iteritems(tasks):\n",
    "    print(\"Spearman correlation of scores on {} {}\".format(name, evaluate_similarity(speech2vec_embeddings, data.X, data.y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
