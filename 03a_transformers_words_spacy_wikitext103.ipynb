{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.learner import *\n",
    "from fastai.data.all import *\n",
    "from fastai.callback.tracker import SaveModelCallback\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib2 import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.nn import MSELoss\n",
    "import logging\n",
    "from six import iteritems\n",
    "from web.datasets.similarity import fetch_MEN, fetch_WS353, fetch_SimLex999\n",
    "from web.embeddings import fetch_GloVe\n",
    "from web.evaluate import evaluate_similarity\n",
    "from web.embedding import Embedding, Vocabulary\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://s3.amazonaws.com/fast-ai-nlp/wikitext-103.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/wikitext-103/train.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29539, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_train[0].tolist()\n",
    "texts = texts[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.attrs import ORTH, LEMMA\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp.tokenizer.add_special_case('<unk>', [{ORTH: \"<sos>\"}])\n",
    "nlp.tokenizer.add_special_case('<eos>', [{ORTH: \"<eos>\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from functools import partial\n",
    "from spacy.util import minibatch\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "batch_size= 200\n",
    "partitions = minibatch(texts, size=batch_size)\n",
    "\n",
    "executor = Parallel(n_jobs=10, backend=\"multiprocessing\", prefer=\"threads\")\n",
    "\n",
    "def transform_texts(nlp, texts):\n",
    "    for i in range(len(texts)):\n",
    "        texts[i] = [line.strip() for line in texts[i].split('\\n') if len(line) > 100]\n",
    "        texts[i] = ('\\n').join(texts[i])\n",
    "    docs = nlp.tokenizer.pipe(texts)\n",
    "    return list(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 4.86 s, total: 1min 10s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "do = delayed(partial(transform_texts, nlp))\n",
    "tasks = (do(batch) for batch in partitions)\n",
    "docs_hierarchical = executor(tasks)\n",
    "\n",
    "docs = []\n",
    "for ds in docs_hierarchical: docs += ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.7 s, sys: 92 ms, total: 16.8 s\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = []\n",
    "\n",
    "for doc in docs:\n",
    "    for token in doc:\n",
    "        if (token.is_alpha or token.is_punct) and '@' not in str(token):\n",
    "            words.append(str(token).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {\n",
    "    \"MEN\": fetch_MEN(),\n",
    "    \"WS353\": fetch_WS353(),\n",
    "    \"SIMLEX999\": fetch_SimLex999()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "\n",
    "for k, v in tasks.items():\n",
    "    for row in v.X:\n",
    "        for w in row:\n",
    "            words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1819"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1305"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([w for w, _ in c.most_common(10_000)]).intersection(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([w for w, _ in c.most_common(20_000)] + ['<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "\n",
    "for doc in docs:\n",
    "    toks = []\n",
    "    for token in doc:\n",
    "        if (token.is_alpha or token.is_punct) and '@' not in str(token):\n",
    "            if str(token) in vocab:\n",
    "                toks.append(str(token).lower())\n",
    "            else:\n",
    "                toks.append('<unk>')\n",
    "    toks += ['<eos>']\n",
    "    text += ' '.join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32496344"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6222584"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = text.split()\n",
    "len(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6222584"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> first published description of the species was written by botanist <unk> <unk> in , based on a specimen '"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[990:1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = set(words)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91955"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'is_alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-71abd1aed9bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m'@-@'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'is_alpha'"
     ]
    }
   ],
   "source": [
    "'@-@'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 41597),\n",
       " (',', 31908),\n",
       " ('.', 23754),\n",
       " ('of', 17399),\n",
       " ('and', 15601),\n",
       " ('in', 14039),\n",
       " ('to', 12953),\n",
       " ('a', 11608),\n",
       " ('\"', 9778),\n",
       " ('was', 7357),\n",
       " ('@-@', 6066),\n",
       " ('on', 5349),\n",
       " ('that', 5061),\n",
       " (\"'s\", 4884),\n",
       " ('\\n', 4706),\n",
       " ('as', 4626),\n",
       " ('for', 4399),\n",
       " ('by', 4229),\n",
       " ('with', 4119),\n",
       " ('he', 3608),\n",
       " ('(', 3568),\n",
       " (')', 3567),\n",
       " ('his', 3166),\n",
       " ('it', 3163),\n",
       " ('is', 2948),\n",
       " ('at', 2928),\n",
       " ('from', 2905),\n",
       " ('>', 2629),\n",
       " ('<', 2455),\n",
       " ('unk', 2424),\n",
       " ('were', 2402),\n",
       " ('an', 2033),\n",
       " ('had', 2019),\n",
       " ('which', 1775),\n",
       " ('be', 1461),\n",
       " ('not', 1459),\n",
       " ('but', 1442),\n",
       " (\"'\", 1407),\n",
       " ('this', 1360),\n",
       " ('one', 1257),\n",
       " ('after', 1255),\n",
       " ('first', 1252),\n",
       " ('its', 1241),\n",
       " ('her', 1227),\n",
       " (';', 1202),\n",
       " ('they', 1190),\n",
       " ('their', 1141),\n",
       " ('also', 1106),\n",
       " ('–', 1097),\n",
       " ('two', 1086),\n",
       " ('have', 1056),\n",
       " ('are', 1043),\n",
       " ('@,@', 1043),\n",
       " ('been', 1007),\n",
       " ('who', 950),\n",
       " ('new', 945),\n",
       " ('or', 940),\n",
       " ('has', 934),\n",
       " ('when', 928),\n",
       " ('@.@', 897),\n",
       " (':', 888),\n",
       " ('she', 885),\n",
       " ('would', 858),\n",
       " ('during', 841),\n",
       " ('i', 823),\n",
       " ('into', 819),\n",
       " ('time', 809),\n",
       " ('other', 808),\n",
       " ('over', 786),\n",
       " ('all', 754),\n",
       " ('more', 708),\n",
       " ('him', 707),\n",
       " ('1', 704),\n",
       " ('no', 694),\n",
       " ('only', 676),\n",
       " ('while', 668),\n",
       " ('later', 665),\n",
       " ('out', 655),\n",
       " ('about', 650),\n",
       " ('up', 647),\n",
       " ('game', 642),\n",
       " ('three', 626),\n",
       " ('between', 608),\n",
       " ('most', 604),\n",
       " ('some', 602),\n",
       " ('made', 594),\n",
       " ('before', 571),\n",
       " ('song', 569),\n",
       " ('there', 562),\n",
       " ('than', 551),\n",
       " ('season', 551),\n",
       " ('year', 541),\n",
       " ('000', 528),\n",
       " ('north', 517),\n",
       " ('such', 517),\n",
       " ('3', 515),\n",
       " ('being', 499),\n",
       " ('years', 495),\n",
       " ('west', 495),\n",
       " ('however', 493),\n",
       " ('2', 491),\n",
       " ('album', 491),\n",
       " ('war', 483),\n",
       " ('/', 478),\n",
       " ('where', 474),\n",
       " ('second', 472),\n",
       " ('them', 470),\n",
       " ('then', 460),\n",
       " ('united', 456),\n",
       " ('number', 447),\n",
       " ('5', 442),\n",
       " ('world', 438),\n",
       " ('film', 438),\n",
       " ('became', 436),\n",
       " ('could', 434),\n",
       " ('state', 432),\n",
       " ('said', 430),\n",
       " ('against', 428),\n",
       " ('several', 426),\n",
       " ('may', 426),\n",
       " ('people', 424),\n",
       " ('used', 423),\n",
       " ('so', 422),\n",
       " ('did', 415),\n",
       " ('through', 414),\n",
       " ('team', 411),\n",
       " ('both', 407),\n",
       " ('states', 406),\n",
       " ('under', 403),\n",
       " ('following', 401),\n",
       " ('m', 399),\n",
       " ('until', 396),\n",
       " ('including', 395),\n",
       " ('part', 393),\n",
       " ('4', 392),\n",
       " ('although', 384),\n",
       " ('these', 381),\n",
       " ('well', 380),\n",
       " ('september', 375),\n",
       " ('many', 374),\n",
       " ('four', 374),\n",
       " ('km', 373),\n",
       " ('american', 372),\n",
       " ('work', 372),\n",
       " ('series', 370),\n",
       " ('day', 368),\n",
       " ('can', 366),\n",
       " ('south', 362),\n",
       " ('line', 362),\n",
       " ('because', 359),\n",
       " ('—', 354),\n",
       " ('you', 354),\n",
       " ('british', 350),\n",
       " ('released', 350),\n",
       " ('began', 347),\n",
       " ('music', 342),\n",
       " ('if', 340),\n",
       " ('around', 338),\n",
       " ('known', 335),\n",
       " ('along', 334),\n",
       " ('us', 334),\n",
       " ('6', 333),\n",
       " ('october', 329),\n",
       " ('castro', 329),\n",
       " ('t', 327),\n",
       " ('court', 317),\n",
       " ('end', 315),\n",
       " ('early', 315),\n",
       " ('10', 311),\n",
       " ('city', 311),\n",
       " ('march', 311),\n",
       " ('great', 310),\n",
       " ('company', 307),\n",
       " ('any', 302),\n",
       " ('another', 299),\n",
       " ('found', 297),\n",
       " ('took', 294),\n",
       " ('episode', 294),\n",
       " ('five', 294),\n",
       " ('government', 294),\n",
       " ('like', 293),\n",
       " ('7', 293),\n",
       " ('long', 291),\n",
       " ('band', 291),\n",
       " ('national', 291),\n",
       " ('called', 290),\n",
       " ('single', 289),\n",
       " ('high', 289),\n",
       " ('john', 289),\n",
       " ('since', 288),\n",
       " ('april', 287),\n",
       " ('group', 285),\n",
       " ('general', 285),\n",
       " ('club', 284),\n",
       " ('august', 283),\n",
       " ('near', 281),\n",
       " ('area', 278),\n",
       " ('storm', 278),\n",
       " ('what', 276),\n",
       " ('same', 274),\n",
       " ('wrote', 270),\n",
       " ('best', 269),\n",
       " ('man', 267),\n",
       " ('back', 266),\n",
       " ('received', 265),\n",
       " ('show', 264),\n",
       " ('family', 263),\n",
       " ('described', 263),\n",
       " ('december', 263),\n",
       " (']', 261),\n",
       " ('8', 260),\n",
       " ('east', 259),\n",
       " ('we', 259),\n",
       " ('...', 258),\n",
       " ('[', 258),\n",
       " ('november', 258),\n",
       " ('just', 255),\n",
       " ('each', 253),\n",
       " ('million', 253),\n",
       " ('down', 251),\n",
       " ('now', 251),\n",
       " ('command', 251),\n",
       " ('&', 251),\n",
       " ('due', 249),\n",
       " ('wallace', 247),\n",
       " ('record', 245),\n",
       " ('large', 244),\n",
       " ('january', 244),\n",
       " ('june', 244),\n",
       " ('trail', 243),\n",
       " ('0', 242),\n",
       " ('name', 241),\n",
       " ('much', 240),\n",
       " ('life', 240),\n",
       " ('february', 240),\n",
       " ('9', 239),\n",
       " ('will', 239),\n",
       " ('members', 238),\n",
       " ('july', 238),\n",
       " ('book', 238),\n",
       " ('use', 237),\n",
       " ('off', 237),\n",
       " ('12', 236),\n",
       " ('u.s.', 235),\n",
       " ('tropical', 235),\n",
       " ('service', 234),\n",
       " ('aircraft', 234),\n",
       " ('public', 234),\n",
       " ('left', 233),\n",
       " ('named', 232),\n",
       " ('own', 232),\n",
       " ('road', 232),\n",
       " ('york', 232),\n",
       " ('air', 231),\n",
       " ('led', 230),\n",
       " ('top', 229),\n",
       " ('included', 228),\n",
       " ('times', 227),\n",
       " ('thomas', 227),\n",
       " ('league', 227),\n",
       " ('late', 226),\n",
       " ('power', 225),\n",
       " ('home', 225),\n",
       " ('place', 224),\n",
       " ('$', 224),\n",
       " ('games', 223),\n",
       " ('published', 221),\n",
       " ('selena', 220),\n",
       " ('species', 219),\n",
       " ('very', 219),\n",
       " ('system', 219),\n",
       " ('character', 218),\n",
       " ('songs', 218),\n",
       " ('ship', 218),\n",
       " ('final', 217),\n",
       " ('release', 217),\n",
       " ('built', 216),\n",
       " ('again', 215),\n",
       " ('way', 215),\n",
       " ('hurricane', 214),\n",
       " ('15', 213),\n",
       " ('side', 212),\n",
       " ('bolton', 212),\n",
       " ('church', 212),\n",
       " ('president', 211),\n",
       " ('played', 211),\n",
       " ('country', 211),\n",
       " ('lake', 210),\n",
       " ('major', 209),\n",
       " ('set', 209),\n",
       " ('20', 209),\n",
       " ('six', 208),\n",
       " ('house', 208),\n",
       " ('water', 208),\n",
       " ('player', 207),\n",
       " ('river', 207),\n",
       " ('third', 207),\n",
       " ('white', 206),\n",
       " ('even', 206),\n",
       " ('still', 206),\n",
       " ('according', 206),\n",
       " ('within', 205),\n",
       " ('100', 205),\n",
       " ('point', 204),\n",
       " ('century', 204),\n",
       " ('don', 203),\n",
       " ('further', 202),\n",
       " ('island', 202),\n",
       " ('military', 201),\n",
       " ('video', 200),\n",
       " ('town', 200),\n",
       " ('held', 199),\n",
       " ('despite', 197),\n",
       " ('do', 196),\n",
       " ('role', 196),\n",
       " ('last', 196),\n",
       " ('stated', 196),\n",
       " ('never', 196),\n",
       " ('fitzgerald', 196),\n",
       " ('based', 194),\n",
       " ('11', 193),\n",
       " ('though', 193),\n",
       " ('16', 193),\n",
       " ('2011', 193),\n",
       " ('30', 192),\n",
       " ('western', 192),\n",
       " ('london', 192),\n",
       " ('h', 192),\n",
       " ('s', 191),\n",
       " ('ft', 190),\n",
       " ('having', 190),\n",
       " ('days', 190),\n",
       " ('law', 190),\n",
       " ('how', 189),\n",
       " ('make', 189),\n",
       " ('williams', 189),\n",
       " ('english', 189),\n",
       " ('salinger', 189),\n",
       " ('old', 188),\n",
       " ('main', 187),\n",
       " ('half', 187),\n",
       " ('original', 186),\n",
       " ('14', 185),\n",
       " ('my', 185),\n",
       " ('school', 185),\n",
       " ('next', 183),\n",
       " ('good', 183),\n",
       " ('gave', 183),\n",
       " ('considered', 182),\n",
       " ('version', 181),\n",
       " ('former', 180),\n",
       " ('lost', 180),\n",
       " ('recorded', 179),\n",
       " ('party', 179),\n",
       " ('13', 178),\n",
       " ('support', 178),\n",
       " ('among', 178),\n",
       " ('death', 178),\n",
       " ('small', 177),\n",
       " ('2009', 177),\n",
       " ('moved', 177),\n",
       " ('career', 177),\n",
       " ('army', 176),\n",
       " ('father', 176),\n",
       " ('story', 175),\n",
       " ('play', 175),\n",
       " ('came', 174),\n",
       " ('park', 174),\n",
       " ('miles', 173),\n",
       " ('paul', 172),\n",
       " ('royal', 172),\n",
       " ('.<eos', 171),\n",
       " ('men', 171),\n",
       " ('highway', 171),\n",
       " ('few', 170),\n",
       " ('produced', 170),\n",
       " ('live', 170),\n",
       " ('soviet', 170),\n",
       " ('written', 168),\n",
       " ('those', 168),\n",
       " ('winds', 168),\n",
       " ('right', 167),\n",
       " ('anti', 166),\n",
       " ('2010', 166),\n",
       " ('little', 165),\n",
       " ('division', 165),\n",
       " ('without', 165),\n",
       " ('should', 165),\n",
       " ('2008', 164),\n",
       " ('per', 164),\n",
       " ('macarthur', 164),\n",
       " ('history', 163),\n",
       " ('won', 163),\n",
       " ('continued', 162),\n",
       " ('week', 162),\n",
       " ('mountain', 162),\n",
       " ('£', 161),\n",
       " ('terry', 161),\n",
       " ('love', 160),\n",
       " ('mph', 160),\n",
       " ('f', 159),\n",
       " ('ming', 159),\n",
       " ('political', 158),\n",
       " ('ten', 158),\n",
       " ('himself', 158),\n",
       " ('german', 158),\n",
       " ('making', 157),\n",
       " ('track', 157),\n",
       " ('19', 157),\n",
       " ('less', 156),\n",
       " ('short', 156),\n",
       " ('2012', 156),\n",
       " ('ii', 155),\n",
       " ('university', 155),\n",
       " ('action', 154),\n",
       " ('2007', 154),\n",
       " ('upon', 154),\n",
       " ('radio', 154),\n",
       " ('forest', 153),\n",
       " ('battle', 153),\n",
       " ('writing', 153),\n",
       " ('chart', 153),\n",
       " ('different', 152),\n",
       " ('given', 152),\n",
       " ('position', 152),\n",
       " ('run', 151),\n",
       " ('24', 151),\n",
       " ('developed', 151),\n",
       " ('me', 151),\n",
       " ('star', 151),\n",
       " ('kakapo', 151),\n",
       " ('range', 150),\n",
       " ('often', 150),\n",
       " ('performance', 150),\n",
       " ('international', 150),\n",
       " ('o', 150),\n",
       " ('central', 149),\n",
       " ('17', 149),\n",
       " ('appeared', 149),\n",
       " ('force', 149),\n",
       " ('michigan', 149),\n",
       " ('form', 148),\n",
       " ('free', 148),\n",
       " ('manager', 148),\n",
       " ('cuba', 148),\n",
       " ('reported', 147),\n",
       " ('low', 147),\n",
       " ('land', 147),\n",
       " ('route', 147),\n",
       " ('america', 146),\n",
       " ('%', 146),\n",
       " ('25', 146),\n",
       " ('months', 146),\n",
       " ('street', 146),\n",
       " ('instead', 145),\n",
       " ('total', 145),\n",
       " ('away', 145),\n",
       " ('return', 145),\n",
       " ('sent', 144),\n",
       " ('something', 144),\n",
       " ('records', 144),\n",
       " ('returned', 144),\n",
       " ('australia', 144),\n",
       " ('reached', 144),\n",
       " ('ships', 144),\n",
       " ('u', 143),\n",
       " ('caused', 142),\n",
       " ('take', 142),\n",
       " ('period', 142),\n",
       " ('section', 141),\n",
       " ('southern', 141),\n",
       " ('conference', 141),\n",
       " ('feet', 140),\n",
       " ('eight', 140),\n",
       " ('king', 140),\n",
       " ('title', 139),\n",
       " ('points', 139),\n",
       " ('once', 139),\n",
       " ('production', 139),\n",
       " ('b', 138),\n",
       " ('griffin', 138),\n",
       " ('tibet', 138),\n",
       " ('noted', 137),\n",
       " ('station', 137),\n",
       " ('de', 137),\n",
       " ('college', 137),\n",
       " ('together', 136),\n",
       " ('french', 136),\n",
       " ('railway', 136),\n",
       " ('order', 135),\n",
       " ('18', 135),\n",
       " ('result', 135),\n",
       " ('died', 135),\n",
       " ('damage', 134),\n",
       " ('member', 134),\n",
       " ('saying', 134),\n",
       " ('lead', 134),\n",
       " ('22', 134),\n",
       " ('young', 134),\n",
       " ('using', 133),\n",
       " ('went', 133),\n",
       " ('madonna', 133),\n",
       " ('win', 133),\n",
       " ('cyclone', 133),\n",
       " ('become', 132),\n",
       " ('relationship', 132),\n",
       " ('saw', 131),\n",
       " ('local', 131),\n",
       " ('abi', 131),\n",
       " ('cunard', 131),\n",
       " ('2006', 130),\n",
       " ('too', 130),\n",
       " ('success', 130),\n",
       " ('throughout', 130),\n",
       " ('coastal', 130),\n",
       " ('molotov', 130),\n",
       " ('allardyce', 130),\n",
       " ('sea', 129),\n",
       " ('close', 129),\n",
       " ('taylor', 129),\n",
       " ('remained', 129),\n",
       " ('opened', 129),\n",
       " ('television', 128),\n",
       " ('development', 127),\n",
       " ('get', 127),\n",
       " ('signed', 127),\n",
       " ('various', 126),\n",
       " ('attack', 126),\n",
       " ('go', 126),\n",
       " ('control', 126),\n",
       " ('england', 126),\n",
       " ('construction', 125),\n",
       " ('squadron', 125),\n",
       " ('critics', 124),\n",
       " ('food', 124),\n",
       " ('50', 124),\n",
       " ('harrison', 124),\n",
       " ('areas', 123),\n",
       " ('building', 123),\n",
       " ('weeks', 123),\n",
       " ('decided', 122),\n",
       " ('post', 122),\n",
       " ('!', 122),\n",
       " ('seven', 122),\n",
       " ('cathedral', 122),\n",
       " ('strong', 121),\n",
       " ('established', 121),\n",
       " ('26', 121),\n",
       " ('2005', 121),\n",
       " ('mm', 121),\n",
       " ('act', 121),\n",
       " ('mount', 120),\n",
       " ('include', 120),\n",
       " ('china', 120),\n",
       " ('case', 120),\n",
       " ('told', 120),\n",
       " ('st.', 120),\n",
       " ('mother', 119),\n",
       " ('hall', 119),\n",
       " ('region', 118),\n",
       " ('40', 118),\n",
       " ('light', 118),\n",
       " ('working', 118),\n",
       " ('design', 118),\n",
       " ('performed', 118),\n",
       " ('magazine', 118),\n",
       " ('far', 118),\n",
       " ('union', 118),\n",
       " ('center', 118),\n",
       " ('atlantic', 118),\n",
       " ('ivan', 118),\n",
       " ('thought', 117),\n",
       " ('hit', 117),\n",
       " ('provided', 117),\n",
       " ('george', 117),\n",
       " ('level', 116),\n",
       " ('similar', 116),\n",
       " ('full', 116),\n",
       " ('county', 116),\n",
       " ('billboard', 116),\n",
       " ('every', 116),\n",
       " ('forces', 116),\n",
       " ('oxford', 116),\n",
       " ('desha', 116),\n",
       " ('black', 115),\n",
       " ('eastern', 115),\n",
       " ('valley', 115),\n",
       " ('australian', 115),\n",
       " ('21', 115),\n",
       " ('’', 115),\n",
       " ('leading', 115),\n",
       " ('campaign', 115),\n",
       " ('felt', 114),\n",
       " ('earlier', 114),\n",
       " ('27', 114),\n",
       " ('son', 113),\n",
       " ('seen', 113),\n",
       " ('started', 113),\n",
       " ('european', 113),\n",
       " ('james', 113),\n",
       " ('recording', 113),\n",
       " ('see', 113),\n",
       " ('bengough', 113),\n",
       " ('previous', 112),\n",
       " ('human', 112),\n",
       " ('formed', 112),\n",
       " ('riot', 112),\n",
       " ('rather', 111),\n",
       " ('characters', 111),\n",
       " ('added', 111),\n",
       " ('possible', 111),\n",
       " ('ball', 111),\n",
       " ('william', 110),\n",
       " ('generally', 110),\n",
       " ('northern', 110),\n",
       " ('operations', 110),\n",
       " ('ever', 110),\n",
       " ('500', 109),\n",
       " ('located', 109),\n",
       " ('studio', 109),\n",
       " ('re', 109),\n",
       " ('award', 109),\n",
       " ('plan', 109),\n",
       " ('adams', 109),\n",
       " ('131', 109),\n",
       " ('taken', 108),\n",
       " ('operation', 108),\n",
       " ('society', 108),\n",
       " ('completed', 108),\n",
       " ('field', 108),\n",
       " ('decision', 108),\n",
       " ('football', 108),\n",
       " ('tibetan', 108),\n",
       " ('pussy', 108),\n",
       " ('playing', 107),\n",
       " ('head', 107),\n",
       " ('non', 107),\n",
       " ('loss', 107),\n",
       " ('born', 107),\n",
       " ('addition', 106),\n",
       " ('followed', 106),\n",
       " ('2014', 106),\n",
       " ('sold', 106),\n",
       " ('chinese', 106),\n",
       " ('body', 105),\n",
       " ('almost', 105),\n",
       " ('23', 105),\n",
       " ('joined', 105),\n",
       " ('popular', 105),\n",
       " ('mark', 105),\n",
       " ('night', 105),\n",
       " ('killed', 105),\n",
       " ('grant', 105),\n",
       " ('200', 104),\n",
       " ('style', 104),\n",
       " ('brought', 104),\n",
       " ('met', 104),\n",
       " ('featured', 104),\n",
       " ('heavy', 104),\n",
       " ('emperor', 104),\n",
       " ('claimed', 104),\n",
       " ('believed', 103),\n",
       " ('created', 103),\n",
       " ('women', 103),\n",
       " ('uk', 103),\n",
       " ('hours', 103),\n",
       " ('soon', 103),\n",
       " ('homer', 103),\n",
       " ('riverina', 103),\n",
       " ('blue', 102),\n",
       " ('28', 102),\n",
       " ('project', 102),\n",
       " ('eventually', 102),\n",
       " ('review', 102),\n",
       " ('works', 102),\n",
       " ('relief', 102),\n",
       " ('carey', 102),\n",
       " ('wake', 102),\n",
       " ('present', 101),\n",
       " ('islands', 101),\n",
       " ('originally', 101),\n",
       " ('able', 101),\n",
       " ('coast', 101),\n",
       " ('god', 101),\n",
       " ('here', 101),\n",
       " ('tour', 101),\n",
       " ('open', 101),\n",
       " ('important', 100),\n",
       " ('age', 100),\n",
       " ('boats', 100),\n",
       " ('thus', 100),\n",
       " ('hot', 100),\n",
       " ('charles', 100),\n",
       " ('spent', 100),\n",
       " ('assembly', 100),\n",
       " ('players', 99),\n",
       " ('staff', 99),\n",
       " ('fire', 99),\n",
       " ('children', 99),\n",
       " ('natural', 99),\n",
       " ('mid', 99),\n",
       " ('forced', 99),\n",
       " ('scene', 99),\n",
       " ('rock', 99),\n",
       " ('robert', 99),\n",
       " ('does', 98),\n",
       " ('come', 98),\n",
       " ('put', 98),\n",
       " ('wife', 98),\n",
       " ('29', 98),\n",
       " ('attempt', 98),\n",
       " ('tech', 98),\n",
       " ('placed', 97),\n",
       " ('interest', 97),\n",
       " ('must', 97),\n",
       " ('asked', 97),\n",
       " ('2000', 97),\n",
       " ('least', 97),\n",
       " ('article', 97),\n",
       " ('mi', 97),\n",
       " ('tons', 97),\n",
       " ('towards', 97),\n",
       " ('france', 97),\n",
       " ('manchester', 97),\n",
       " ('change', 96),\n",
       " ('our', 96),\n",
       " ('give', 96),\n",
       " ('available', 96),\n",
       " ('david', 95),\n",
       " ('site', 95),\n",
       " ('material', 95),\n",
       " ('replaced', 95),\n",
       " ('served', 95),\n",
       " ('nine', 95),\n",
       " ('georgia', 95),\n",
       " ('across', 94),\n",
       " ('st', 94),\n",
       " ('wanted', 94),\n",
       " ('training', 94),\n",
       " ('council', 94),\n",
       " ('class', 94),\n",
       " ('shot', 93),\n",
       " ('?', 93),\n",
       " ('praised', 92),\n",
       " ('taking', 92),\n",
       " ('worked', 92),\n",
       " ('month', 92),\n",
       " ('involved', 92),\n",
       " ('previously', 92),\n",
       " ('bridge', 92),\n",
       " ('stage', 92),\n",
       " ('office', 92),\n",
       " ('religious', 92),\n",
       " ('tintin', 92),\n",
       " ('defense', 92),\n",
       " ('ground', 91),\n",
       " ('help', 91),\n",
       " ('stories', 91),\n",
       " ('features', 91),\n",
       " ('lower', 91),\n",
       " ('supported', 91),\n",
       " ('singer', 91),\n",
       " ('others', 91),\n",
       " ('captain', 91),\n",
       " ('weather', 91),\n",
       " ('freeway', 91),\n",
       " ('nations', 91),\n",
       " ('khánh', 91),\n",
       " ('association', 90),\n",
       " ('2013', 90),\n",
       " ('co', 90),\n",
       " ('hergé', 90),\n",
       " ('darwin', 90),\n",
       " ('planché', 90),\n",
       " ('base', 89),\n",
       " ('goal', 89),\n",
       " ('official', 89),\n",
       " ('europe', 89),\n",
       " ('chief', 89),\n",
       " ('finished', 89),\n",
       " ('writes', 89),\n",
       " ('particularly', 89),\n",
       " ('guns', 89),\n",
       " ('contract', 89),\n",
       " ('business', 89),\n",
       " ('election', 89),\n",
       " ('increased', 88),\n",
       " ('find', 88),\n",
       " ('idea', 88),\n",
       " ('beginning', 88),\n",
       " ('big', 88),\n",
       " ('start', 88),\n",
       " ('crew', 88),\n",
       " ('matter', 87),\n",
       " ('bbc', 87),\n",
       " ('your', 87),\n",
       " ('damaged', 87),\n",
       " ('appointed', 87),\n",
       " ('grand', 87),\n",
       " ('kenya', 87),\n",
       " ('henry', 86),\n",
       " ('population', 86),\n",
       " ('special', 86),\n",
       " ('announced', 86),\n",
       " ('yet', 86),\n",
       " ('parts', 86),\n",
       " ('carried', 86),\n",
       " ('front', 86),\n",
       " ('rights', 86),\n",
       " ('centre', 86),\n",
       " ('traffic', 86),\n",
       " ('wright', 86),\n",
       " ('suggested', 85),\n",
       " ('example', 85),\n",
       " ('designed', 85),\n",
       " ('2002', 85),\n",
       " ('regular', 85),\n",
       " ('movement', 85),\n",
       " ('avenue', 85),\n",
       " ('passed', 85),\n",
       " ('above', 85),\n",
       " ('fleet', 85),\n",
       " ('rest', 85),\n",
       " ('iii', 85),\n",
       " ('foreign', 85),\n",
       " ('peter', 85),\n",
       " ('board', 85),\n",
       " ('1941', 85),\n",
       " ('nba', 85),\n",
       " ('lakers', 85),\n",
       " ('d', 84),\n",
       " ('port', 84),\n",
       " ('art', 84),\n",
       " ('moving', 84),\n",
       " ('bay', 84),\n",
       " ('introduced', 84),\n",
       " ('going', 84),\n",
       " ('britain', 84),\n",
       " ('cover', 84),\n",
       " ('married', 84),\n",
       " ('italian', 84),\n",
       " ('whether', 84),\n",
       " ('becoming', 84),\n",
       " ('authority', 84),\n",
       " ('successful', 83),\n",
       " ('outside', 83),\n",
       " ('either', 83),\n",
       " ('ended', 83),\n",
       " ('positive', 83),\n",
       " ('significant', 83),\n",
       " ('kingdom', 83),\n",
       " ('peak', 83),\n",
       " ('latin', 83),\n",
       " ('destroyed', 83),\n",
       " ('x', 83),\n",
       " ('allowed', 83),\n",
       " ('proposed', 83),\n",
       " ('real', 82),\n",
       " ('leader', 82),\n",
       " ('behind', 82),\n",
       " ('test', 82),\n",
       " ('complete', 82),\n",
       " ('fact', 82),\n",
       " ('ran', 82),\n",
       " ('peaked', 82),\n",
       " ('failed', 82),\n",
       " ('scored', 82),\n",
       " ('governor', 82),\n",
       " ('passenger', 82),\n",
       " ('common', 81),\n",
       " ('2003', 81),\n",
       " ('better', 81),\n",
       " ('move', 81),\n",
       " ('latter', 81),\n",
       " ('issued', 81),\n",
       " ('singles', 81),\n",
       " ('michael', 81),\n",
       " ('news', 81),\n",
       " ('1995', 81),\n",
       " ('pressure', 81),\n",
       " ('media', 81),\n",
       " ('daily', 81),\n",
       " ('required', 81),\n",
       " ('boat', 81),\n",
       " ('canada', 80),\n",
       " ('list', 80),\n",
       " ('initially', 80),\n",
       " ('friend', 80),\n",
       " ('planned', 80),\n",
       " ('canadian', 80),\n",
       " ('un', 80),\n",
       " ('raf', 80),\n",
       " ('mexico', 79),\n",
       " ('press', 79),\n",
       " ('network', 79),\n",
       " ('red', 79),\n",
       " ('know', 79),\n",
       " ('1999', 79),\n",
       " ('mega', 79),\n",
       " ('fifth', 79),\n",
       " ('1943', 79),\n",
       " ('truman', 79),\n",
       " ('letter', 78),\n",
       " ('covered', 78),\n",
       " ('summer', 78),\n",
       " ('experience', 78),\n",
       " ('already', 78),\n",
       " ('industry', 78),\n",
       " ('31', 78),\n",
       " ('writer', 78),\n",
       " ('reviews', 78),\n",
       " ('ordered', 78),\n",
       " ('view', 78),\n",
       " ('fourth', 78),\n",
       " ('highest', 78),\n",
       " ('average', 78),\n",
       " ('officers', 78),\n",
       " ('washington', 77),\n",
       " ('surface', 77),\n",
       " ('prior', 77),\n",
       " ('themselves', 77),\n",
       " ('entire', 77),\n",
       " ('buildings', 77),\n",
       " ('subsequently', 77),\n",
       " ('2001', 77),\n",
       " ('60', 77),\n",
       " ('course', 77),\n",
       " ('dropped', 77),\n",
       " ('texas', 77),\n",
       " ('russian', 77),\n",
       " ('brill', 77),\n",
       " ('victory', 76),\n",
       " ('groups', 76),\n",
       " ('300', 76),\n",
       " ('resulted', 76),\n",
       " ('interview', 76),\n",
       " ('director', 76),\n",
       " ('appearance', 76),\n",
       " ('hand', 76),\n",
       " ('navy', 76),\n",
       " ('issue', 76),\n",
       " ('coach', 76),\n",
       " ('1940', 76),\n",
       " ('1942', 76),\n",
       " ('kolb', 76),\n",
       " ('might', 75),\n",
       " ('shortly', 75),\n",
       " ('plans', 75),\n",
       " ('cross', 75),\n",
       " ('got', 75),\n",
       " ('policy', 75),\n",
       " ('rihanna', 75),\n",
       " ('reynolds', 75),\n",
       " ('length', 74),\n",
       " ('2004', 74),\n",
       " ('directed', 74),\n",
       " ('sound', 74),\n",
       " ('turned', 74),\n",
       " ('quickly', 74),\n",
       " ('leaving', 74),\n",
       " ('really', 74),\n",
       " ('changes', 74),\n",
       " ('trade', 74),\n",
       " ('opening', 74),\n",
       " ('debut', 74),\n",
       " ('person', 74),\n",
       " ('police', 74),\n",
       " ('awards', 74),\n",
       " ('melbourne', 74),\n",
       " ('arthur', 74),\n",
       " ('race', 74),\n",
       " ('raaf', 74),\n",
       " ('championship', 74),\n",
       " ('report', 73),\n",
       " ('whom', 73),\n",
       " ('agreed', 73),\n",
       " ('future', 73),\n",
       " ('stone', 73),\n",
       " ('always', 73),\n",
       " ('finally', 73),\n",
       " ('personal', 73),\n",
       " ('lines', 73),\n",
       " ('voice', 73),\n",
       " ('need', 73),\n",
       " ('services', 73),\n",
       " ('legitimate', 73),\n",
       " ('florida', 73),\n",
       " ('committee', 73),\n",
       " ('castle', 73),\n",
       " ('parkway', 73),\n",
       " ('likely', 72),\n",
       " ('launched', 72),\n",
       " ('middle', 72),\n",
       " ('think', 72),\n",
       " ('event', 72),\n",
       " ('modern', 72),\n",
       " ('stating', 72),\n",
       " ('conditions', 72),\n",
       " ('janet', 72),\n",
       " ('ben', 72),\n",
       " ...]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.most_common(20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "657120"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_hierarchical[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = ''\n",
    "\n",
    "for text in texts:\n",
    "    all_texts += text + '<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://github.com/karpathy/minGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 3000000 characters, 224 unique.\n",
      "data has 544234 characters, 127 unique.\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "\n",
    "train_dataset = CharDataset(all_texts[:3_000_000], block_size)\n",
    "valid_dataset = CharDataset(all_texts[3_000_000:], block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class GPTConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class GPT1Config(GPTConfig):\n",
    "    \"\"\" GPT-1 like network roughly 125M params \"\"\"\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    n_embd = 768\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # input embedding stem\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.block_size = config.block_size\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \"\"\"\n",
    "        This long function is unfortunately doing something very simple and is being very defensive:\n",
    "        We are separating out all parameters of the model into two buckets: those that will experience\n",
    "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
    "        We are then returning the PyTorch optimizer object.\n",
    "        \"\"\"\n",
    "\n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # special case the position embedding parameter in the root GPT module as not decayed\n",
    "        no_decay.add('pos_emb')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "\n",
    "        # forward the GPT model\n",
    "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
    "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
    "        x = self.drop(token_embeddings + position_embeddings)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\"\"\"\n",
    "Simple training loop; Boilerplate that could apply to any arbitrary neural network,\n",
    "so nothing in this file really has anything to do with GPT specifically.\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import logging\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    batch_size = 64\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = None\n",
    "    num_workers = 0 # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_dataset, test_dataset, config):\n",
    "        self.model = model\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
    "        torch.save(raw_model.state_dict(), self.config.ckpt_path)\n",
    "\n",
    "    def train(self):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            data = self.train_dataset if is_train else self.test_dataset\n",
    "            loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
    "                                batch_size=config.batch_size,\n",
    "                                num_workers=config.num_workers)\n",
    "\n",
    "            losses = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "            for it, (x, y) in pbar:\n",
    "\n",
    "                # place data on the correct device\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    logits, loss = model(x, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "\n",
    "                if is_train:\n",
    "\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
    "\n",
    "            if not is_train:\n",
    "                test_loss = float(np.mean(losses))\n",
    "                logger.info(\"test loss: %f\", test_loss)\n",
    "                return test_loss\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            run_epoch('train')\n",
    "            if self.test_dataset is not None:\n",
    "                test_loss = run_epoch('test')\n",
    "\n",
    "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
    "            good_model = self.test_dataset is None or test_loss < best_loss\n",
    "            if self.config.ckpt_path is not None and good_model:\n",
    "                best_loss = test_loss\n",
    "                self.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 3510: train loss 0.87360. lr 5.673885e-04:  30%|██▉       | 3511/11719 [30:52<1:12:20,  1.89it/s]"
     ]
    }
   ],
   "source": [
    "tconf = TrainerConfig(max_epochs=2, batch_size=256, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=8)\n",
    "trainer = Trainer(model, train_dataset, valid_dataset, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    v, ix = torch.topk(logits, k)\n",
    "    out = logits.clone()\n",
    "    out[out < v[:, [-1]]] = -float('Inf')\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
    "    \"\"\"\n",
    "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
    "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
    "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
    "    of block_size, unlike an RNN that has an infinite context window.\n",
    "    \"\"\"\n",
    "    block_size = model.get_block_size()\n",
    "    model.eval()\n",
    "    for k in range(steps):\n",
    "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
    "        logits, _ = model(x_cond)\n",
    "        # pluck the logits at the final step and scale by temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop probabilities to only the top k options\n",
    "        if top_k is not None:\n",
    "            logits = top_k_logits(logits, top_k)\n",
    "        # apply softmax to convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution or take the most likely\n",
    "        if sample:\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
    "        # append to the sequence and continue\n",
    "        x = torch.cat((x, ix), dim=1)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Tony Stark constructed his first rocket at the age of 7.\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
