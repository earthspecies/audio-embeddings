{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.learner import *\n",
    "from fastai.data.all import *\n",
    "from fastai.callback.tracker import SaveModelCallback\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib2 import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.5 s, sys: 2.3 s, total: 26.8 s\n",
      "Wall time: 26.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17937758, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv('data/examples.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While attempting to train the model, it turned out that the reading and unpickling operation done at this scale, with so many files, is very computationally expensive.\n",
    "\n",
    "But the examples in the mfcc represenatations are very small. Let's read them all into the memory before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniq_fns = np.unique(df.target_fn.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# fn2feature = {}\n",
    "# for fn in uniq_fns:\n",
    "#     ary = pd.read_pickle(f'data/examples/{fn}.pkl')\n",
    "#     fn2feature[fn] = ary\n",
    "\n",
    "# pd.to_pickle(fn2feature, 'data/fn2feature.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 s, sys: 9.91 s, total: 25.3 s\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fn2features = pd.read_pickle('data/fn2feature.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(df.source_word.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# counter = Counter(df.source_word)\n",
    "\n",
    "# most_common_words = set([t[0] for t in counter.most_common(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_word</th>\n",
       "      <th>target_word</th>\n",
       "      <th>source_fn</th>\n",
       "      <th>target_fn</th>\n",
       "      <th>set_name</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>distance_from_target</th>\n",
       "      <th>audio_fpath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>FELT</td>\n",
       "      <td>8af4aebcf4a74004b02db9f88d99e89a</td>\n",
       "      <td>1cb9442ec1a6468282da309756e2ff57</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>1</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>THAT</td>\n",
       "      <td>8af4aebcf4a74004b02db9f88d99e89a</td>\n",
       "      <td>2f60546c930c47068ee0a129e6d51c39</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>2</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FELT</td>\n",
       "      <td>I</td>\n",
       "      <td>1cb9442ec1a6468282da309756e2ff57</td>\n",
       "      <td>8af4aebcf4a74004b02db9f88d99e89a</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>1</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FELT</td>\n",
       "      <td>THAT</td>\n",
       "      <td>1cb9442ec1a6468282da309756e2ff57</td>\n",
       "      <td>2f60546c930c47068ee0a129e6d51c39</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>1</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FELT</td>\n",
       "      <td>IT</td>\n",
       "      <td>1cb9442ec1a6468282da309756e2ff57</td>\n",
       "      <td>155ad336d88c4cbf814a1237983b5b18</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>2</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_word target_word                         source_fn  \\\n",
       "0           I        FELT  8af4aebcf4a74004b02db9f88d99e89a   \n",
       "1           I        THAT  8af4aebcf4a74004b02db9f88d99e89a   \n",
       "2        FELT           I  1cb9442ec1a6468282da309756e2ff57   \n",
       "3        FELT        THAT  1cb9442ec1a6468282da309756e2ff57   \n",
       "4        FELT          IT  1cb9442ec1a6468282da309756e2ff57   \n",
       "\n",
       "                          target_fn         set_name  speaker_id  book_id  \\\n",
       "0  1cb9442ec1a6468282da309756e2ff57  train-clean-360        7000    83696   \n",
       "1  2f60546c930c47068ee0a129e6d51c39  train-clean-360        7000    83696   \n",
       "2  8af4aebcf4a74004b02db9f88d99e89a  train-clean-360        7000    83696   \n",
       "3  2f60546c930c47068ee0a129e6d51c39  train-clean-360        7000    83696   \n",
       "4  155ad336d88c4cbf814a1237983b5b18  train-clean-360        7000    83696   \n",
       "\n",
       "   distance_from_target  \\\n",
       "0                     1   \n",
       "1                     2   \n",
       "2                     1   \n",
       "3                     1   \n",
       "4                     2   \n",
       "\n",
       "                                                        audio_fpath  \n",
       "0  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  \n",
       "1  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  \n",
       "2  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  \n",
       "3  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  \n",
       "4  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.96 s, sys: 164 ms, total: 5.12 s\n",
      "Wall time: 5.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "unique_fns = df.source_fn.unique()\n",
    "np.random.shuffle(unique_fns)\n",
    "lengths = []\n",
    "for i, features in enumerate(fn2features.values()):\n",
    "    lengths.append(features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.028019713968394"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f812b583650>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(features.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mean = -5\n",
    "dataset_std = 15\n",
    "\n",
    "def normalize_data(ary):\n",
    "    return (ary - dataset_mean) / dataset_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_list(): return list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# word2row_idxs = defaultdict(empty_list)\n",
    "\n",
    "# for idx, row in df.iterrows():\n",
    "#     word2row_idxs[row.source_word].append(idx)\n",
    "    \n",
    "# pd.to_pickle(word2row_idxs, 'data/word2row_idxs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2row_idxs = pd.read_pickle('data/word2row_idxs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(fn, pad_to=max(lengths), pad_left=False):\n",
    "    ary = fn2features[fn][:pad_to]\n",
    "    example = np.zeros((pad_to, 13))\n",
    "    if pad_left:\n",
    "        example[-ary.shape[0]:, :] = ary\n",
    "    else: example[:ary.shape[0], :] = ary\n",
    "    return example.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.nan in vocab: vocab.remove(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, n):\n",
    "        self.vocab = vocab * n\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    def __getitem__(self, idx):\n",
    "        row_idx = np.random.randint(len(word2row_idxs[self.vocab[idx]]))\n",
    "        source_fn = df.source_fn[row_idx]\n",
    "        target_fn = df.target_fn[row_idx]\n",
    "        x = normalize_data(prepare_features(source_fn, pad_left=True))\n",
    "        y = normalize_data(prepare_features(target_fn))\n",
    "        return np.stack((x, y)), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 2048\n",
    "LR = 1e-3\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "train_dl = DataLoader(Dataset(270), BS, NUM_WORKERS, shuffle=True)\n",
    "valid_dl = DataLoader(Dataset(30), BS, NUM_WORKERS)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got the following error while training:\n",
    "\n",
    "# DataLoader worker (pid 2073) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.\n",
    "# trying the solution I found here: https://github.com/pytorch/pytorch/issues/5040\n",
    "# which is to execute\n",
    "!sudo umount /dev/shm/ && sudo mount -t tmpfs -o rw,nosuid,nodev,noexec,relatime,size=50G shm /dev/shm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bidirectional encoder, 1 layer, concatenate hidden state\n",
    "class Model(Module):\n",
    "    def __init__(self, hidden_size=25, num_layers_encoder=3):\n",
    "        self.return_embeddings = False\n",
    "        self.num_layers_encoder = num_layers_encoder\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.encoder= nn.LSTM(\n",
    "            input_size=13,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=self.num_layers_encoder,\n",
    "            batch_first=True,\n",
    "            dropout=0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=2*hidden_size+13,\n",
    "            hidden_size=2*hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.lin = nn.Linear(2*hidden_size, 13)\n",
    "            \n",
    "    def forward(self, source_and_target_features):\n",
    "        source_features = source_and_target_features[:, 0]\n",
    "        target_features = source_and_target_features[:, 1]\n",
    "        _, (embeddings, _) = self.encoder(source_features)\n",
    "        \n",
    "        embeddings = torch.cat((embeddings[-1], embeddings[-2]), 1)\n",
    "        if self.return_embeddings: return embeddings\n",
    "        \n",
    "        target_features = torch.cat((torch.zeros(target_features.shape[0], 1, 13).cuda(), target_features), 1)\n",
    "        inputs = torch.cat(\n",
    "            (\n",
    "                target_features[:, :-1, :],\n",
    "                embeddings.unsqueeze(1).repeat(1, target_features.shape[1]-1, 1)\n",
    "            ), 2)\n",
    "        x, _ = self.decoder(inputs, (embeddings.unsqueeze(0), torch.zeros_like(embeddings).unsqeeze(0)))\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse_loss = MSELoss()\n",
    "# def targ_trunc_MSE(preds, targs):\n",
    "#     return mse_loss(preds, targs[:, 1:, :])\n",
    "\n",
    "learn = Learner(dls.cuda(), Model().cuda(), loss_func=MSELoss(), lr=1e-3, opt_func=Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/12 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='8789' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/8789 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(12, cbs=SaveModelCallback(fname='1e-3_Adam_tf', every_epoch=True), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dl:\n",
    "#     break\n",
    "\n",
    "# o = learn.model(batch[0].cuda())\n",
    "# plt.imshow(o[0][:5, :].detach().cpu())\n",
    "\n",
    "# plt.imshow(batch[1][0][:5, :].detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate embedding for each unique word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_utterances = df[df.set_name.isin(['train-clean-360', 'train-clean-100', 'dev-clean'])].drop_duplicates(['source_fn'])\n",
    "df_unique_utterances.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetAllUtterances():\n",
    "    def __len__(self):\n",
    "        return df_unique_utterances.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        source_fn = df_unique_utterances.iloc[idx].source_fn\n",
    "        target_fn = df_unique_utterances.iloc[idx].target_fn\n",
    "        x = normalize_data(prepare_features(source_fn, pad_left=True))\n",
    "        y = normalize_data(prepare_features(target_fn))\n",
    "        return np.stack((x, y)), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dl = DataLoader(DatasetAllUtterances(), BS, NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "learn.model.return_embeddings = True\n",
    "learn.model.train = False\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "with torch.no_grad():    \n",
    "    for batch in all_dl:\n",
    "        embeddings = learn.model(batch[0].cuda())\n",
    "        all_embeddings.append(embeddings.detach().cpu().squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = torch.cat(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.return_embeddings = False\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(all_dl):\n",
    "        outputs = learn.model(batch[0].cuda())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(outputs[31].cpu().numpy().T[:, :20])\n",
    "axs[1].imshow(batch[1][31].cpu().numpy().T[:, :20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(outputs[0].cpu().numpy().T[:, :20])\n",
    "axs[1].imshow(batch[1][0].cpu().numpy().T[:, :20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(outputs[30].cpu().numpy().T[:, :20])\n",
    "axs[1].imshow(batch[1][30].cpu().numpy().T[:, :20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(outputs[15].cpu().numpy().T[:, :20])\n",
    "axs[1].imshow(batch[1][15].cpu().numpy().T[:, :20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# word2row_idxs_unique_utterances = defaultdict(empty_list)\n",
    "\n",
    "# for idx, row in df_unique_utterances.iterrows():\n",
    "#     word2row_idxs_unique_utterances[row.source_word].append(idx)\n",
    "    \n",
    "# pd.to_pickle(word2row_idxs_unique_utterances, 'word2row_idxs_unique_utterances.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2row_idxs_unique_utterances = pd.read_pickle('word2row_idxs_unique_utterances.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embedding = {}\n",
    "\n",
    "for k, v in word2row_idxs_unique_utterances.items():\n",
    "    word2embedding[k] = all_embeddings[np.array(v)].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embedding_without_nans= {}\n",
    "nans_encountered = 0\n",
    "for k, v in word2embedding.items():\n",
    "    if k in vocab and k == k and (not np.isnan(v.numpy()).any()):\n",
    "        word2embedding_without_nans[k] = v.numpy()\n",
    "    else: nans_encountered += 1\n",
    "\n",
    "print(f'Encountered rows with nan values: {nans_encountered}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings():\n",
    "    def __init__(self, embeddings, index2word):\n",
    "        '''embeddings - numpy array of embeddings, index2word - list of words corresponding to embeddings'''\n",
    "        assert len(embeddings) == len(index2word)\n",
    "        self.vectors = embeddings\n",
    "        self.i2w = index2word\n",
    "        self.w2i = {w:i for i, w in enumerate(index2word)}\n",
    "            \n",
    "    def analogy(self, a, b, c, n=5, discard_question_words=True):\n",
    "        '''\n",
    "        a is to b as c is to ?\n",
    "        \n",
    "        Performs the following algebraic calculation: result = emb_a - emb_b + emb_c\n",
    "        Looks up n closest words to result.\n",
    "        \n",
    "        Implements the embedding space math behind the famous word2vec example:\n",
    "        king - man + woman = queen\n",
    "        '''\n",
    "        question_word_indices = [self.w2i[word] for word in [a, b, c]]\n",
    "        a, b, c = [self.vectors[idx] for idx in question_word_indices] \n",
    "        result = a - b + c\n",
    "        \n",
    "        if discard_question_words: return self.nn_words_to(result, question_word_indices, n)\n",
    "        else:                      return self.nn_words_to(result, n=n)\n",
    "        \n",
    "    def nn_words_to(self, vector, skip_indices=[], n=5):\n",
    "        nn_indices = self.word_idxs_ranked_by_cosine_similarity_to(vector)\n",
    "        nn_words = []\n",
    "        for idx in nn_indices:\n",
    "            if idx in skip_indices: continue\n",
    "            nn_words.append(self.i2w[idx])\n",
    "            if len(nn_words) == n: break\n",
    "        \n",
    "        return nn_words\n",
    "    \n",
    "    def word_idxs_ranked_by_cosine_similarity_to(self, vector):\n",
    "        return np.flip(\n",
    "            np.argsort(self.vectors @ vector / (self.vectors_lengths() * np.linalg.norm(vector, axis=-1)))\n",
    "        )\n",
    "    \n",
    "    def vectors_lengths(self):\n",
    "        if not hasattr(self, 'vectors_length_cache'):\n",
    "            self.vectors_length_cache = np.linalg.norm(self.vectors, axis=-1)\n",
    "        return self.vectors_length_cache\n",
    "    \n",
    "    def __getitem__(self, word):\n",
    "        return self.vectors[self.w2i[word]]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_txt_file(cls, path_to_txt_file, limit=None):\n",
    "        '''create embeddings from word2vec embeddings text file'''\n",
    "        index, vectors = [], []\n",
    "        with open(path_to_txt_file) as f:\n",
    "            f.readline() # discarding the header line\n",
    "            for line in f:\n",
    "                try:\n",
    "                    embedding = np.array([float(s) for s in line.split()[1:]])\n",
    "                    if embedding.shape[0] != 300: continue\n",
    "                    vectors.append(embedding)\n",
    "                    index.append(line.split()[0])\n",
    "                except ValueError: pass # we may have encountered a 2 word embedding, for instance 'New York' or 'w dolinie'\n",
    "                if limit is not None and len(vectors) == limit: break\n",
    "        return cls(np.stack(vectors), index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Embeddings(\n",
    "    np.array(list(word2embedding_without_nans.values())),\n",
    "    [w.lower() for w in list(word2embedding_without_nans.keys())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.nn_words_to(e['fast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.nn_words_to(e['lost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.nn_words_to(e['true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.nn_words_to(e['virtue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.nn_words_to(e['germany'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.nn_words_to(e['crazy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.nn_words_to(e['slow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating embeddings using [word-embeddings-benchmarks](https://github.com/kudkudak/word-embeddings-benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from six import iteritems\n",
    "from web.datasets.similarity import fetch_MEN, fetch_WS353, fetch_SimLex999\n",
    "from web.embeddings import fetch_GloVe\n",
    "from web.evaluate import evaluate_similarity\n",
    "from web.embedding import Embedding, Vocabulary\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {\n",
    "    \"MEN\": fetch_MEN(),\n",
    "    \"WS353\": fetch_WS353(),\n",
    "    \"SIMLEX999\": fetch_SimLex999()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_embeddings = Embedding(\n",
    "    Vocabulary([w.lower() for w in list(word2embedding_without_nans.keys())]),\n",
    "    np.array(list(word2embedding_without_nans.values()))\n",
    ")\n",
    "\n",
    "speech2vec = KeyedVectors.load_word2vec_format('../speech2vec-pretrained-vectors/speech2vec/50.vec', binary=False) \n",
    "speech2vec_embeddings = Embedding(Vocabulary(list(speech2vec.vocab.keys())), speech2vec.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in iteritems(tasks):\n",
    "    print(\"Spearman correlation of scores on {} {}\".format(name, evaluate_similarity(our_embeddings, data.X, data.y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in iteritems(tasks):\n",
    "    print(\"Spearman correlation of scores on {} {}\".format(name, evaluate_similarity(speech2vec_embeddings, data.X, data.y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
