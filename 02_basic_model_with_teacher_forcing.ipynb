{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.learner import *\n",
    "from fastai.data.all import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib2 import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 s, sys: 2.72 s, total: 28.8 s\n",
      "Wall time: 30.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17937758, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv('data/examples.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While attempting to train the model, it turned out that the reading and unpickling operation done at this scale, with so many files, is very computationally expensive.\n",
    "\n",
    "But the examples in the mfcc represenatations are very small. Let's read them all into the memory before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_fns = np.unique(df.target_fn.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000003ccee914766a64101e306727a7d',\n",
       "       '0000043abbe34d9eb4444e71f42d98ec',\n",
       "       '00000670bdea4bd19dae463ef7588299',\n",
       "       '000008ccba824ba0a23db3ced81ed136',\n",
       "       '000009eefcd0426393fae6d776a3743d'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_fns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 9s, sys: 5min 5s, total: 27min 14s\n",
      "Wall time: 1h 10min 23s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# fn2feature = {}\n",
    "# for fn in uniq_fns:\n",
    "#     ary = pd.read_pickle(f'data/examples/{fn}.pkl')\n",
    "#     fn2feature[fn] = ary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_pickle(fn2feature, 'data/fn2feature.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.2 s, sys: 10.9 s, total: 27 s\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fn2features = pd.read_pickle('data/fn2feature.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suspect that reading the data from a file saved within numpy (`numpy.save`) is much less computationally expensive than unpickling it, but I might be wrong. Either way, at ~4 million of unique utterances, the dataset is small enough to comfortably fit within memory of a GCP instance (at ~53GBs used RAM during training).\n",
    "\n",
    "This might not be ideal for experimentation on home rigs. Saving the data using `numpy.save` and evaluating performance would definitely be a very interesting and useful exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_word</th>\n",
       "      <th>target_word</th>\n",
       "      <th>source_fn</th>\n",
       "      <th>target_fn</th>\n",
       "      <th>set_name</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>distance_from_target</th>\n",
       "      <th>audio_fpath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>FELT</td>\n",
       "      <td>43a14fe66b6348718f089ec553bf57b1</td>\n",
       "      <td>b6c1288c07504236a2b054c972a6e92f</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>1</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>THAT</td>\n",
       "      <td>43a14fe66b6348718f089ec553bf57b1</td>\n",
       "      <td>ed9ceac76f7e49e7809ec56db40d37a2</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>2</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FELT</td>\n",
       "      <td>I</td>\n",
       "      <td>b6c1288c07504236a2b054c972a6e92f</td>\n",
       "      <td>43a14fe66b6348718f089ec553bf57b1</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>1</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FELT</td>\n",
       "      <td>THAT</td>\n",
       "      <td>b6c1288c07504236a2b054c972a6e92f</td>\n",
       "      <td>ed9ceac76f7e49e7809ec56db40d37a2</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>1</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FELT</td>\n",
       "      <td>IT</td>\n",
       "      <td>b6c1288c07504236a2b054c972a6e92f</td>\n",
       "      <td>235da027aeac418da43430e1f3470da4</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>2</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_word target_word                         source_fn  \\\n",
       "0           I        FELT  43a14fe66b6348718f089ec553bf57b1   \n",
       "1           I        THAT  43a14fe66b6348718f089ec553bf57b1   \n",
       "2        FELT           I  b6c1288c07504236a2b054c972a6e92f   \n",
       "3        FELT        THAT  b6c1288c07504236a2b054c972a6e92f   \n",
       "4        FELT          IT  b6c1288c07504236a2b054c972a6e92f   \n",
       "\n",
       "                          target_fn         set_name  speaker_id  book_id  \\\n",
       "0  b6c1288c07504236a2b054c972a6e92f  train-clean-360        7000    83696   \n",
       "1  ed9ceac76f7e49e7809ec56db40d37a2  train-clean-360        7000    83696   \n",
       "2  43a14fe66b6348718f089ec553bf57b1  train-clean-360        7000    83696   \n",
       "3  ed9ceac76f7e49e7809ec56db40d37a2  train-clean-360        7000    83696   \n",
       "4  235da027aeac418da43430e1f3470da4  train-clean-360        7000    83696   \n",
       "\n",
       "   distance_from_target  \\\n",
       "0                     1   \n",
       "1                     2   \n",
       "2                     1   \n",
       "3                     1   \n",
       "4                     2   \n",
       "\n",
       "                                                        audio_fpath  \n",
       "0  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  \n",
       "1  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  \n",
       "2  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  \n",
       "3  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  \n",
       "4  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = df[df.set_name.isin(['train-clean-360', 'train-clean-100', 'dev-clean'])]\n",
    "valid_examples = df[df.set_name == 'test-clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159688530, 1751292)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples.size, valid_examples.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.02 s, sys: 204 ms, total: 5.22 s\n",
      "Wall time: 5.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "unique_fns = df.source_fn.unique()\n",
    "np.random.shuffle(unique_fns)\n",
    "lengths = []\n",
    "for i, features in enumerate(fn2features.values()):\n",
    "    lengths.append(features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5e23719750>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAD4CAYAAABsdWSLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAR9UlEQVR4nO3deYxdd3nG8eeZffOWOHaCHbApIW2g0NApZQlLE5DCooRKSE1UqrRFslQVCBRKg5CK2r9QiyioRVArAaIShVYhhSilNGmAAlJIGUIAG2cxWSd2YidOHHvGs915+8dci8HM2OnvPXPuGfh+pNHMXc7v995zz33m3O09jggBAP5/ujpdAACsRoQnABQgPAGgAOEJAAUITwAo0FPnZL19wzEwtKHOKRsr3OkKqhFdFdyQCv6F/7Kszyp4voJBGvAhnEpuR/KGTE8+pdnpiSW3rlrDc2Bog85/zXtSY4TzjxInP55VRWBEFfv8DQiMmeH8DWn152/IfG96iGruk6QqAqNrJj9Gz3QF6ZkcoooaPJ8b465vfHLZyxqwuQDA6kN4AkABwhMAChCeAFAgFZ62L7Z9j+29tq+qqigAaLri8LTdLelTkt4k6TxJl9s+r6rCAKDJMnueL5e0NyLuj4gZSV+UdGk1ZQFAs2XCc4ukRxadHm+f93Ns77A9ZntsdmYiMR0ANEcmPJf6ZPMvfCI1InZGxGhEjPb2DSemA4DmyITnuKSzF53eKmlfrhwAWB0y4fk9SefY3m67T9Jlkm6qpiwAaLbi77ZHxJztd0n6L0ndkj4bEbsrqwwAGizVGCQivirpqxXVAgCrBt8wAoAChCcAFKi1n2d0WXODubzurqDH3+CDR1LLe2o6XYN686s+urvzdST/fU6dn29u/cQFs+kxzth8OD3G7Fx+fR6dGEgt7658Q8+Yz+8TnXFT7nZI0tDjucaih7f3p2vI9keN7uV7zbLnCQAFCE8AKEB4AkABwhMAChCeAFCA8ASAAoQnABQgPAGgAOEJAAUITwAoQHgCQAHCEwAKEJ4AUIDwBIAChCcAFCA8AaBArc2Q57ulqfW5vJ7vzdfR9/RgavmefN9dtUbyjV7n+/PNe1vJMWaHl28W+2x1H8pvhr1fOz09xsBMvhFx77rc+pxZm1+fsyPpIbT/DblGxpLkydyDdeTB/LqI5ENk/iSbJnueAFCA8ASAAoQnABQgPAGgQHF42j7b9jds77G92/aVVRYGAE2WeZtzTtL7I+JO22skfd/2rRHxk4pqA4DGKt7zjIj9EXFn++8jkvZI2lJVYQDQZJW85ml7m6TzJd2xxGU7bI/ZHpubmqhiOgDouHR42h6R9CVJ742IZ068PCJ2RsRoRIz2DAxnpwOARkiFp+1eLQTndRFxYzUlAUDzZd5tt6RrJO2JiI9XVxIANF9mz/PVkv5I0oW272r/vLmiugCg0Yo/qhQR35GU/+Y+AKxCfMMIAAoQngBQoNZ+nps2P6V3feBLqTH2Tm1O1/Gvt1yQWr73cK4fqCTNrYn0GPkRpOjJjdL/ZL6G4Ufy/8P3v6aVHmNwX/7hsPahXE/Q9T+dS9dwdEv+dkxuq+AVuXWzqcWPnZnfLlr9ue17vm/5y9jzBIAChCcAFCA8AaAA4QkABQhPAChAeAJAAcITAAoQngBQgPAEgAKEJwAUIDwBoADhCQAFCE8AKEB4AkABwhMAChCeAFCg1mbI+49s0N/e9vupMbom83n/3P/ONWmd78k3ip3clF/13TP5dsit3txtmTwzXYI27ppOj7Hl1iPpMTw1kx5Dk8dSi7fO2pguoWtmID3G5psfTY8xf8b61PLRm98uojuXF4cOLd/cmj1PAChAeAJAAcITAAoQngBQIB2etrtt/8D2zVUUBACrQRV7nldK2lPBOACwaqTC0/ZWSW+RdHU15QDA6pDd8/yEpA9KWv7DUADwS6g4PG2/VdKBiPj+Ka63w/aY7bHW0aOl0wFAo2T2PF8t6RLbD0r6oqQLbX/hxCtFxM6IGI2I0e6RkcR0ANAcxeEZER+KiK0RsU3SZZK+HhHvqKwyAGgwPucJAAUqaQwSEd+U9M0qxgKA1YA9TwAoQHgCQAHCEwAK1NoM2b0tDWyeSI1x2shkuo4HNpyWWt69rXQNzz3rYHqMh/ZuSo/RfzC3CZz+u4+laxhfuzk9xpnfzTXelaTeo3PpMfofeCK1fNeD+9I1DEyenh5j77u2p8eYHck16153T37fbmZdbvnZh5avgT1PAChAeAJAAcITAAoQngBQgPAEgAKEJwAUIDwBoADhCQAFCE8AKEB4AkABwhMAChCeAFCA8ASAAoQnABQgPAGgAOEJAAXqbYZ8pFu931mbGuOpwdzykjSYvNWzw7kmr5L02OCa9Bh9h7rzYxzOLd/7j/nGu9sfP5Iew1P5RsZV7EpET/I+ifl0Da17f5oeY/sN/ekxoje3QrumZtM1TJ01klp+fHL5xzp7ngBQgPAEgAKEJwAUIDwBoEAqPG2vt32D7btt77H9yqoKA4Amy77b/klJX4uIt9vukzRUQU0A0HjF4Wl7raTXSvpjSYqIGUkz1ZQFAM2Wedr+fEkHJX3O9g9sX217+MQr2d5he8z2WOvYRGI6AGiOTHj2SHqZpE9HxPmSJiRddeKVImJnRIxGxGj34C9kKwCsSpnwHJc0HhF3tE/foIUwBYBfesXhGRGPSXrE9rntsy6S9JNKqgKAhsu+2/5uSde132m/X9Kf5EsCgOZLhWdE3CVptKJaAGDV4BtGAFCA8ASAArX285zvkya25voVtta18oXMObV4/8F8H83ppwbSY3hr/jsJre25++PIi/Lron9N7v6QJN29IT3E+nvzfVpP//ajqeVjPl+De3rTY9z/B/m+uUrelP6n8tvFxJbc9j2ze/ka2PMEgAKEJwAUIDwBoADhCQAFCE8AKEB4AkABwhMAChCeAFCA8ASAAoQnABQgPAGgAOEJAAUITwAoQHgCQAHCEwAKEJ4AUKDWZsjqDrXW5poZezLffLdrJtdk1fP5Jq0D+/MNa3sm0kOoaza3/DMvnU7X8Lpte9Nj/PMFt6fHuHc2v0L/4oG3p5bf89AL0jXEZP5hvfae/DbePZXrhjy1MV2CnO8tvSz2PAGgAOEJAAUITwAoQHgCQIFUeNp+n+3dtnfZvt52/pCQALAKFIen7S2S3iNpNCJeLKlb0mVVFQYATZZ92t4jadB2j6QhSfvyJQFA8xWHZ0Q8Kuljkh6WtF/S4Yi4parCAKDJMk/bN0i6VNJ2Sc+RNGz7HUtcb4ftMdtjrSMVfKobABog87T9DZIeiIiDETEr6UZJrzrxShGxMyJGI2K0e81wYjoAaI5MeD4s6RW2h2xb0kWS9lRTFgA0W+Y1zzsk3SDpTkk/bo+1s6K6AKDRUh0EIuIjkj5SUS0AsGrwDSMAKEB4AkABwhMACtTbDLkr1DU4lxrCw7nlJWnNyLH0GFkDfckuxJKOHMu3EpicyjVlXnf7ULqGW479ZnqMF+w9Nz3G/KH+9BhZA4/nm33PDeU7AHs+PYSiJ9dQ+djW/GNd2dtxkm7K7HkCQAHCEwAKEJ4AUIDwBIAChCcAFCA8AaAA4QkABQhPAChAeAJAAcITAAoQngBQgPAEgAKEJwAUIDwBoADhCQAFCE8AKFBvM+SWNT+Ra77b83S+WezhNX25Gg7n/+ccm801ipWkVn++6W3PsVwd87lVKUka3JffDNd9O3+fTG7KjzH4RK77bquvgvt0Kj/G5KYKts9kn+yhh/PbRfd0bvkD08uvB/Y8AaAA4QkABQhPAChAeAJAgVOGp+3P2j5ge9ei806zfavt+9q/N6xsmQDQLM9mz/Pzki4+4byrJN0WEedIuq19GgB+ZZwyPCPiW5IOnXD2pZKubf99raS3VVwXADRa6WuemyNivyS1f29a7oq2d9gesz3WOjpROB0ANMuKv2EUETsjYjQiRrtHhld6OgCoRWl4Pm77LElq/z5QXUkA0Hyl4XmTpCvaf18h6SvVlAMAq8Oz+ajS9ZJul3Su7XHb75T0UUlvtH2fpDe2TwPAr4xTfvM+Ii5f5qKLKq4FAFYNvmEEAAUITwAoUGs/z/4nQ+dcm2uw58j3Ktz/qpHU8sOP5Xo2StKxjRX0S+zLj9H/dG59PvnbrXQN63flN8P1u59Oj7HhP/alx2g980xq+e4zzkjXMPOis9Nj9B3N3yfHTsv13j3yvPz2veah3GO1a/Ykl6VGBoBfUYQnABQgPAGgAOEJAAUITwAoQHgCQAHCEwAKEJ4AUIDwBIAChCcAFCA8AaAA4QkABQhPAChAeAJAAcITAAoQngBQoNZmyK3+Lj3z/MHUGL3H8s2Qp0/PjdEzmW/SOrMuPYT6n8yPMTuSvC35u0OtgfwY912VH2RoaEt6jIHezanlDx5Ym65B07kmxJJ05rfy+1Uz63LbVmswv3Flt+84yapkzxMAChCeAFCA8ASAAoQnABQ4ZXja/qztA7Z3LTrv723fbftHtv/d9vqVLRMAmuXZ7Hl+XtLFJ5x3q6QXR8RLJN0r6UMV1wUAjXbK8IyIb0k6dMJ5t0TEXPvkdyVtXYHaAKCxqnjN808l/edyF9reYXvM9tjc9EQF0wFA56XC0/aHJc1Jum6560TEzogYjYjRnv7hzHQA0BjF3zCyfYWkt0q6KCIq+J4JAKweReFp+2JJfyXpdRExWW1JANB8z+ajStdLul3SubbHbb9T0j9JWiPpVtt32f7MCtcJAI1yyj3PiLh8ibOvWYFaAGDV4BtGAFCA8ASAAoQnABRwnZ8ysn1Q0kMnucpGSU/UVM7JNKGOJtQgNaOOJtQgNaMOaviZOup4XkScsdQFtYbnqdgei4hR6mhGDU2powk1NKUOamhOHTxtB4AChCcAFGhaeO7sdAFtTaijCTVIzaijCTVIzaiDGn6mo3U06jVPAFgtmrbnCQCrAuEJAAUaE562L7Z9j+29tq/qwPxn2/6G7T22d9u+su4aFtXSbfsHtm/uYA3rbd/QPlbVHtuv7FAd72vfH7tsX297oIY5lzpu12m2b7V9X/v3hg7VUevxw5aqYdFlH7AdtjeuZA0nq8P2u9u5sdv23610HYs1Ijxtd0v6lKQ3STpP0uW2z6u5jDlJ74+I35D0Ckl/3oEajrtS0p4OzX3cJyV9LSJ+XdJLO1GP7S2S3iNpNCJeLKlb0mU1TP15/eJxu66SdFtEnCPptvbpTtRR9/HDlqpBts+W9EZJD6/w/MvWYfv3JF0q6SUR8SJJH6upFkkNCU9JL5e0NyLuj4gZSV/UwkqpTUTsj4g7238f0UJYbKmzBkmyvVXSWyRdXffci2pYK+m1anfPioiZiHi6Q+X0SBq03SNpSNK+lZ5wqeN2aWF7vLb997WS3taJOuo+ftgy60KS/kHSByXV8o7zMnX8maSPRsR0+zoH6qjluKaE5xZJjyw6Pa4OBNdxtrdJOl/SHR2Y/hNa2CjnOzD3cc+XdFDS59ovH1xtu/ZjqETEo1rYm3hY0n5JhyPilrrraNscEfvbde2XtKlDdSx20uOHrRTbl0h6NCJ+WPfcJ3ihpNfYvsP2/9j+nTonb0p4eonzOvIZKtsjkr4k6b0R8UzNc79V0oGI+H6d8y6hR9LLJH06Is6XNKF6nqb+nPbripdK2i7pOZKGbb+j7jqa6NkcP2yF5h2S9GFJf13nvMvokbRBCy+z/aWkf7O9VJasiKaE57iksxed3qoanp6dyHavFoLzuoi4se75Jb1a0iW2H9TCSxcX2v5CB+oYlzQeEcf3vG/QQpjW7Q2SHoiIgxExK+lGSa/qQB2S9LjtsySp/bvWp4iLLTp+2B924Phhv6aFf2Y/bG+nWyXdafvMmuuQFrbTG2PB/2rh2dqKv3l1XFPC83uSzrG93XafFt4UuKnOAtr/sa6RtCciPl7n3MdFxIciYmtEbNPCOvh6RNS+pxURj0l6xPa57bMukvSTuuvQwtP1V9geat8/F6lzb6TdJOmK9t9XSPpKJ4pYdPywSzpx/LCI+HFEbIqIbe3tdFzSy9rbTN2+LOlCSbL9Qkl9qrPbU0Q04kfSm7Xw7uFPJX24A/NfoIWXCn4k6a72z5s7uD5eL+nmDs7/W5LG2uvjy5I2dKiOv5F0t6Rdkv5FUn8Nc16vhddYZ7UQDu+UdLoW3mW/r/37tA7VsVcL7w8c30Y/U3cNJ1z+oKSNHVoXfZK+0N427pR0YZ3bJl/PBIACTXnaDgCrCuEJAAUITwAoQHgCQAHCEwAKEJ4AUIDwBIAC/weNbL+GdivSrAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(features.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(fn, pad_to=max(lengths)):\n",
    "    ary = fn2features[fn]\n",
    "    example = np.zeros((pad_to, 13))\n",
    "    example[:ary.shape[0], :] = ary\n",
    "    return example.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = Datasets(\n",
    "    df,\n",
    "    [lambda row: prepare_features(row.source_fn),\n",
    "     lambda row: prepare_features(row.target_fn),\n",
    "     lambda row: prepare_features(row.target_fn)],\n",
    "    n_inp=2,\n",
    "    splits = [train_examples.index, valid_examples.index]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 2048\n",
    "LR = 1e-3\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "train_dl = DataLoader(dss.train, BS, NUM_WORKERS, shuffle=True)\n",
    "valid_dl = DataLoader(dss.valid, BS, NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dl: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(Module):\n",
    "    def __init__(self, hidden_size=50):\n",
    "        self.encoder= nn.LSTM(\n",
    "            input_size=13,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=13,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.lin = nn.Linear(2*hidden_size, 13)\n",
    "            \n",
    "    def forward(self, source_features, target_features):\n",
    "        _, (embeddings, _) = self.encoder(source_features)\n",
    "        embeddings_bi = torch.cat((embeddings, torch.flip(embeddings, (2,))))\n",
    "        x, _ = self.decoder(target_features, (embeddings_bi, torch.zeros_like(embeddings_bi)))\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "\n",
    "learn = Learner(dls.cuda(), Model().cuda(), loss_func=MSELoss(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.011743</td>\n",
       "      <td>0.011293</td>\n",
       "      <td>45:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>0.008639</td>\n",
       "      <td>46:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.007835</td>\n",
       "      <td>0.007880</td>\n",
       "      <td>45:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.007409</td>\n",
       "      <td>0.008419</td>\n",
       "      <td>45:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models/basic_model_4epochs_1e-3.pth')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn.save('basic_model_4epochs_1e-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner at 0x7f5f62fecf10>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load('basic_model_4epochs_1e-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dl = DataLoader(dss, BS, NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    learn.model.train = False\n",
    "    for batch in all_dl:\n",
    "        _, (embeddings, _) = learn.model.encoder(batch[0].cuda())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9804,  0.5087, -0.9841,  ..., -0.9235,  0.9040, -0.0262],\n",
       "         [ 0.9804,  0.5087, -0.9841,  ..., -0.9235,  0.9040, -0.0262],\n",
       "         [ 0.9804,  0.5087, -0.9841,  ..., -0.9235,  0.9040, -0.0262],\n",
       "         ...,\n",
       "         [ 0.9804,  0.5087, -0.9841,  ..., -0.9235,  0.9040, -0.0262],\n",
       "         [ 0.9804,  0.5087, -0.9841,  ..., -0.9235,  0.9040, -0.0262],\n",
       "         [ 0.9804,  0.5087, -0.9841,  ..., -0.9235,  0.9040, -0.0262]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, this model isn't learning anything useful. For any of the inputs,the decoder will output the same embeddings.\n",
    "\n",
    "There might be other issues causing this, but my guess is that this is related to teacher forcing all the way throughout the training.\n",
    "\n",
    "The next steps will certainly consist of replacing the cudnn optimized prediction loop for the decoder and replacing it with something of our own creation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
