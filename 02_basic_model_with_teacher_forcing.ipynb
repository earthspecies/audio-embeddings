{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.learner import *\n",
    "from fastai.data.all import *\n",
    "from fastai.callback.tracker import SaveModelCallback\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib2 import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.nn import MSELoss\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 4s, sys: 2.7 s, total: 1min 7s\n",
      "Wall time: 1min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4140463, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv('data/examples.csv', converters={'target_fns':ast.literal_eval})\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While attempting to train the model, it turned out that the reading and unpickling operation done at this scale, with so many files, is very computationally expensive.\n",
    "\n",
    "But the examples in the mfcc represenatations are very small. Let's read them all into the memory before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# all_fns = []\n",
    "# for l in df.target_fns:\n",
    "#     all_fns += l\n",
    "# all_fns += df.source_fn.values.tolist()    \n",
    "\n",
    "# fn2features = {}\n",
    "# for fn in set(all_fns):\n",
    "#     ary = pd.read_pickle(f'data/examples/{fn}.pkl')\n",
    "#     fn2features[fn] = ary\n",
    "\n",
    "# pd.to_pickle(fn2features, 'data/fn2feature.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.4 s, sys: 16.1 s, total: 35.5 s\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fn2features = pd.read_pickle('data/fn2feature.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4688767"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fn2features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f262b08d710>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(fn2features[list(fn2features.keys())[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suspect that reading the data from a file saved within numpy (`numpy.save`) is much less computationally expensive than unpickling it, but I might be wrong. Either way, at ~4 million of unique utterances, the dataset is small enough to comfortably fit within memory of a GCP instance (at ~53GBs used RAM during training).\n",
    "\n",
    "This might not be ideal for experimentation on home rigs. Saving the data using `numpy.save` and evaluating performance would definitely be a very interesting and useful exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_word</th>\n",
       "      <th>target_word</th>\n",
       "      <th>source_fn</th>\n",
       "      <th>target_fns</th>\n",
       "      <th>set_name</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>audio_fpath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>THAT</td>\n",
       "      <td>['I', 'FELT', 'IT', 'WAS']</td>\n",
       "      <td>8e6739008aaa44feae36b2c3d9c8c44e</td>\n",
       "      <td>[6b23b5c2091b4899b73f8691a9bb3bb9, e717579ac2e04163aa7afe440af37705, 4d18cbcd200c4dd59efa56ba59d6e46b, dee9407548754f009db42e86a4ff5f23]</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IT</td>\n",
       "      <td>['FELT', 'THAT', 'WAS', 'TIME']</td>\n",
       "      <td>4d18cbcd200c4dd59efa56ba59d6e46b</td>\n",
       "      <td>[e717579ac2e04163aa7afe440af37705, 8e6739008aaa44feae36b2c3d9c8c44e, dee9407548754f009db42e86a4ff5f23, 525d4e83b9b548dbaef7d35a77df1afc]</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WAS</td>\n",
       "      <td>['THAT', 'IT', 'TIME', 'TO']</td>\n",
       "      <td>dee9407548754f009db42e86a4ff5f23</td>\n",
       "      <td>[8e6739008aaa44feae36b2c3d9c8c44e, 4d18cbcd200c4dd59efa56ba59d6e46b, 525d4e83b9b548dbaef7d35a77df1afc, 9e1c98df449f48d2aa09db0dcebc7cf4]</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TIME</td>\n",
       "      <td>['IT', 'WAS', 'TO', 'INTRODUCE']</td>\n",
       "      <td>525d4e83b9b548dbaef7d35a77df1afc</td>\n",
       "      <td>[4d18cbcd200c4dd59efa56ba59d6e46b, dee9407548754f009db42e86a4ff5f23, 9e1c98df449f48d2aa09db0dcebc7cf4, c342b93295d644e088fa7ab14465a5b8]</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TO</td>\n",
       "      <td>['WAS', 'TIME', 'INTRODUCE', 'MYSELF']</td>\n",
       "      <td>9e1c98df449f48d2aa09db0dcebc7cf4</td>\n",
       "      <td>[dee9407548754f009db42e86a4ff5f23, 525d4e83b9b548dbaef7d35a77df1afc, c342b93295d644e088fa7ab14465a5b8, 390a5367ec794ed1b73c3ffce4c3aad9]</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_word                             target_word  \\\n",
       "0        THAT              ['I', 'FELT', 'IT', 'WAS']   \n",
       "1          IT         ['FELT', 'THAT', 'WAS', 'TIME']   \n",
       "2         WAS            ['THAT', 'IT', 'TIME', 'TO']   \n",
       "3        TIME        ['IT', 'WAS', 'TO', 'INTRODUCE']   \n",
       "4          TO  ['WAS', 'TIME', 'INTRODUCE', 'MYSELF']   \n",
       "\n",
       "                          source_fn  \\\n",
       "0  8e6739008aaa44feae36b2c3d9c8c44e   \n",
       "1  4d18cbcd200c4dd59efa56ba59d6e46b   \n",
       "2  dee9407548754f009db42e86a4ff5f23   \n",
       "3  525d4e83b9b548dbaef7d35a77df1afc   \n",
       "4  9e1c98df449f48d2aa09db0dcebc7cf4   \n",
       "\n",
       "                                                                                                                                 target_fns  \\\n",
       "0  [6b23b5c2091b4899b73f8691a9bb3bb9, e717579ac2e04163aa7afe440af37705, 4d18cbcd200c4dd59efa56ba59d6e46b, dee9407548754f009db42e86a4ff5f23]   \n",
       "1  [e717579ac2e04163aa7afe440af37705, 8e6739008aaa44feae36b2c3d9c8c44e, dee9407548754f009db42e86a4ff5f23, 525d4e83b9b548dbaef7d35a77df1afc]   \n",
       "2  [8e6739008aaa44feae36b2c3d9c8c44e, 4d18cbcd200c4dd59efa56ba59d6e46b, 525d4e83b9b548dbaef7d35a77df1afc, 9e1c98df449f48d2aa09db0dcebc7cf4]   \n",
       "3  [4d18cbcd200c4dd59efa56ba59d6e46b, dee9407548754f009db42e86a4ff5f23, 9e1c98df449f48d2aa09db0dcebc7cf4, c342b93295d644e088fa7ab14465a5b8]   \n",
       "4  [dee9407548754f009db42e86a4ff5f23, 525d4e83b9b548dbaef7d35a77df1afc, c342b93295d644e088fa7ab14465a5b8, 390a5367ec794ed1b73c3ffce4c3aad9]   \n",
       "\n",
       "          set_name  speaker_id  book_id  \\\n",
       "0  train-clean-360        7000    83696   \n",
       "1  train-clean-360        7000    83696   \n",
       "2  train-clean-360        7000    83696   \n",
       "3  train-clean-360        7000    83696   \n",
       "4  train-clean-360        7000    83696   \n",
       "\n",
       "                                                        audio_fpath  \n",
       "0  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  \n",
       "1  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  \n",
       "2  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  \n",
       "3  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  \n",
       "4  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = df[df.set_name.isin(['train-clean-360', 'train-clean-100', 'dev-clean'])]\n",
    "valid_examples = df[df.set_name == 'test-clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32786448, 337256)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples.size, valid_examples.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.34 s, sys: 116 ms, total: 4.45 s\n",
      "Wall time: 4.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "unique_fns = df.source_fn.unique()\n",
    "np.random.shuffle(unique_fns)\n",
    "lengths = []\n",
    "for i, features in enumerate(fn2features.values()):\n",
    "    lengths.append(features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.019326189593126"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f25009fda50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAACrCAYAAACQVaxZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAR80lEQVR4nO3de4yc5XXH8d+Zmb15vbbX2AbfwtWKkhAUkIWapiJpCBWhqNA7SKlIW4n+0bQQRWqglUpaqVLUpiiVWqVKAylVaVAEpKAqbbESUIJKCWAcwNiEa4yNsY2N1zu76925nP6xQ7te9nKemXd3eNzvR7J2Z+b4meedZ+bMu+/MeY+5uwAAeSp1ewIAgPaRxAEgYyRxAMgYSRwAMkYSB4CMVZbzzsqDg15ZuzYUa0vwpRm3hOCU2BRJ456u3xxaigc35bFKuP/TdQlOV0u1XkswbsqrYPLA/rfcff1cty1rEq+sXavNn785FFuqxzfRGrG4Rl98JbwcDpVXElY44W8fLy3FO1nxQ6aypHfTGE9410+6/2Ybk1lMyhqkzHUp9nxSLcHaJm1Wwnql5JjwmiXMNWW7Xr7lCz+d7zYOpwBAxjpK4mZ2pZm9YGYvmdktRU0KABDTdhI3s7Kkv5P0aUkflHS9mX2wqIkBABbXyZ74pZJecvdX3H1K0j2SrilmWgCAiE6S+GZJr8+4vL913SnM7EYze9LMnmyMjXVwdwCA2TpJ4nN9tPuuz1vd/evuvt3dt5cHBzu4OwDAbJ0k8f2Sts64vEXSG51NBwCQopMk/oSkbWZ2rpn1SrpO0oPFTAsAENF2sY+7183sc5L+U1JZ0p3uvruwmQEAFtVRxaa7f1fSd8P/oeRqDMZKqq6+dGd42NFafyjuhw9/ODxmM6Gaqtm/FGV9Wprq9HLChqVUn/UlPAbBUjWvJfyhmBLaFyzxleQJm1XpjY1bSqjELZfjE5iajL+cKz3xx6DZjD8RK5XYfFN60UxN9oRjz1hTDcdOTMXH7euph2OjJmvFFMxTsQkAGSOJA0DGSOIAkDGSOABkjCQOABkjiQNAxkjiAJAxkjgAZIwkDgAZI4kDQMaWt9t9b0PDm0dCsWP1vvC4zWB9em04XmpcHo13Sl6xL/4w9o6GQ1WeiNUmV06m1MfHQ0u1eGxlMj5wvS+271BqJDQ/ji+tGn3x9ar3J5ScB9ehmfCqqw3G77+0Mh7b6I3PoZQwXw+uQ++J+Ji9KSX6zYFw7MlN8XGrg8G1XRE/TUJpMOEFttA4hYwCAOiKTnpsbjWzh81sj5ntNrObipwYAGBxnRxOqUv6grvvNLMhSU+Z2Q53f76guQEAFtH2nri7H3T3na3fRyXt0Rw9NgEAS6eQY+Jmdo6kiyU9Psdt/9souT4yXsTdAQBaOk7iZrZS0n2Sbnb3d33mPLNRcmX1ik7vDgAwQ0dJ3Mx6NJ3A73b3+4uZEgAgqpNvp5ikOyTtcffbi5sSACCqkz3xj0n6LUmfNLNdrX9XFTQvAEBAJ93uH9XStPIFAAQta9l9s1nS+MmEet+g0VqsRP/Wj/9beMxL+18Nx455vGv27skt4dhnqltDcYcnV4bHnGrEl/zIxGA4duRk/DQJY2P9objGaPxxtVpCefxYPLZ8MqHsfSoWa/HK7CTlk/HYwYNLc0qDWvApM55Q8t5MSBmNzfEHYeuZb4dj+yuxEvkNA/HzavSV4g/snQvcRtk9AGSMJA4AGSOJA0DGSOIAkDGSOABkjCQOABkjiQNAxkjiAJAxkjgAZGxZKzYlqdmIvW888vK28JirhmLnKb/9+9eGx7SE5qxKiK0PJFTKRSv7Et6KPSG2kdD0NYWtngrFrdtyPD5mwoKdtTJeVXdkPF61WgrO4fhYvJnvxIF4Na73JDwRU3prNxLOrhEcN2Wu/esmwrG1o/HH9uhY/NTYExOxstEXxuKlqCvWFtNfgT1xAMgYSRwAMlZEZ5+ymT1tZvGzSwEAClHEnvhNmm6SDABYZp22Z9si6RclfaOY6QAAUnS6J/5VSX8kad6vMczsdt84Mdbh3QEAZuqkx+bVkg67+1MLxc3sdl9eFf+6FgBgcZ322PwlM3tN0j2a7rX5z4XMCgAQ0nYSd/db3X2Lu58j6TpJ33f3zxQ2MwDAovieOABkrJCye3d/RNIji8Y1TLUTsfLVFeviJakbh2Jl1L0/H2+MuufQWeHY+qvx0ujSZDhUClY7Rxv0pt7/VDP+Ht9IOJ2AH4s1VX6rHr//Sl+86Wx1It7U+bx1R8OxGwdOhOLO3hQf87KL94Zj15fir5kXa+vCsSmawf3Cx6oXhMd8uRqfa2lj/Hn4mxueCMf+6srY2r7ViH9544Hq+eHYGxe4jT1xAMgYSRwAMkYSB4CMkcQBIGMkcQDIGEkcADJGEgeAjJHEASBjJHEAyBhJHAAytrzd7t1ktdj7RvPHq8PDvj4aix3eWwuPec6BeEf05jOPhWPLa+Lbpc2x0v+pDfFT/LrFS/Trg+VwbGU8Xvbe7Ik9B+oD8adns6cnHFuenPf09+/y5oahcOz+gdhj+6P4w6p7ej8Zjh3fFN8u743HRk//IEk2GVtb74vff2XVVDi2Phl/cJ94Ll72/qf7Ys/F8snwkOo/Fj9FgPTovLewJw4AGeu0PdsaM7vXzPaa2R4z+2hREwMALK7Twyl/I+k/3P3XzKxX0ooC5gQACGo7iZvZKkmXSfqsJLn7lKT4wSsAQMc6OZxynqQjkr5pZk+b2TfMjCaaALCMOkniFUmXSPqau18saUzSLbODTul2X612cHcAgNk6SeL7Je1398dbl+/VdFI/xSnd7lfGO+AAABbXSaPkNyW9bmbvb111uaTnC5kVACCk02+n/IGku1vfTHlF0m93PiUAQFRHSdzdd0naXtBcAACJlrfsvimVJoJluZV4SeqJD8XK6ce2x0vDVwzGa40bjQ+FYyvleLnxupWxztlDPcfDY6a47IwXw7Gry/FO62cF53tWOdZhXJKONOLl8ef1HAvHPjK+LRw73uwNxe2ubg6PufNwPLanGT862vD483tqKp4mpiZipz+wE/HTJNi+gXDsipH4dk1sjOeD8fNj3562Svz1PTqRkH6/Of9NlN0DQMZI4gCQMZI4AGSMJA4AGSOJA0DGSOIAkDGSOABkjCQOABkjiQNAxkjiAJCx5S27lySLldMPHIq/v/SOxMqdy5PhIVXvj5f6TlwUb3FdSijLLZdiscPD8ZL3SnBMSdpd3RSOPXQyXva+7/iaUNzYaH94zGZCl/Mk9fjzcM1zsZdT74n4KSXGz4mXkffEztIgSRr+ST0+bjUeOzkcK6ev98e3a3J1OFQDR+PP703/FW9EZrXYuLVVsVwkSbWEU3vsW+A29sQBIGOddrv/vJntNrPnzOxbZhbfdQIAdKztJG5mmyX9oaTt7n6hpLKk64qaGABgcZ0eTqlIGjCziqQVkt7ofEoAgKhO2rMdkPQVTR9zPyhpxN0fmh13SqPksYRPXgAAi+rkcMqwpGsknStpk6RBM/vM7LhTGiUPDrY/UwDAu3RyOOVTkl519yPuXpN0v6SfLWZaAICITpL4Pkk/Y2YrzMw03e1+TzHTAgBEdHJM/HFJ90raKenZ1lhfL2heAICATrvd3ybptoLmAgBItLxl9y5ZPVZqevzD8VJf9cW6Vg+ujpfHj70Z/xDWRuOdu70WL7WtBrt87+oZDo9ZmozfvydUsve9HR+3Hj2jwap4CbWV46XspYQ1KG2aCMfWL6+G4vr74+d/qL0dP52BeuLd23uvOh6OrTfjf7DX67GU0kgYc9Ng7HGVpAMj8Rr9UU94LY7EnrReS3ge9sXL/nXvAuPERwEAvNeQxAEgYyRxAMgYSRwAMkYSB4CMkcQBIGMkcQDIGEkcADJGEgeAjC1vxabFqwDLQ7XwsEMrY1V1k7X45q4/++1w7PhkvDnqQG98u0bH+0JxlVK8Sqxei5dh1kdi9y9J9VXhUPUcj81hxYGESsGV8ftX/OHSZG9Ks+ZYVV91dbyyctPWo+HY1X3xiuSRyfh2DffHq1avOHNvKG5tJV6FOVSOb9fVF+wPxz49Ga/KXlOKPQZnV+Jre6QZfyJ+YIHb2BMHgIyRxAEgY4smcTO708wOm9lzM65ba2Y7zOzF1s/4GZgAAIWJ7In/o6QrZ113i6Tvufs2Sd9rXQYALLNFk7i7/0DSsVlXXyPprtbvd0m6tuB5AQAC2j0mfqa7H5Sk1s8N8wXS7R4Als6Sf7BJt3sAWDrtJvFDZrZRklo/Dxc3JQBAVLtJ/EFJN7R+v0HSA8VMBwCQIvIVw29JekzS+81sv5n9rqQvS7rCzF6UdEXrMgBgmS1ah+7u189z0+XJ9+aSBatSV/0gXhZcH4iVOzc+Gi/1HR2P3//UyXg5/9ih+OcCPSPB8vSfhocMP/6S1KzEG8n2jMVLiJvBsxSUJxNOJ3AiHKqBY/EGzBNnxP9Y7anG5msJHaiPn31WOPbIqvjjVT4ZX9vR0XCoRl57Xyiu7+346Scm1sdPa3HHsfi4lbF4M/bK8fFQnJfjazu1IeUzwlvnvYWKTQDIGEkcADJGEgeAjJHEASBjJHEAyBhJHAAyRhIHgIyRxAEgYyRxAMgYSRwAMrbs3e6jbxvHLomXxJarsVLXC9bHO4ev7YuV2UrS0ZPx8tm+SkKpb7BG/o2LVofHXDsQ364UR8bi7eajBd9WipfHl5rx/ZFST7w0u3p8KBw7dTR2qgarx0veh14Jh2rgSDx2bEs81hN29Y5+IPZarA/FB+07Fn+8Gtv6wrG9o/FYq8de41Pxl6Ia8TN7TPdPmwd74gCQsXYbJf+Vme01s2fM7DtmtmZppwkAmEu7jZJ3SLrQ3S+S9BMtdIotAMCSaatRsrs/5O7vHNz9b0kJR9gAAEUp4pj470j69/lunNkouVmlUTIAFKmjJG5mfyKpLunu+WJmNkouraRRMgAUqe2vGJrZDZKulnS5u8dbigAACtNWEjezKyV9UdLH3X1pvngMAFhUu42S/1bSkKQdZrbLzP5+iecJAJhDu42S71iCuQAAEtlyHs42syOSZvdmXyfprWWbxPJhu/LCduXl/9t2ne3u6+f6D8uaxOecgNmT7r69q5NYAmxXXtiuvLBd/4dzpwBAxkjiAJCx90IS/3q3J7BE2K68sF15Ybtaun5MHADQvvfCnjgAoE0kcQDIWFeTuJldaWYvmNlLZnZLN+dSJDN7zcyebVWzPtnt+bRrnoYga81sh5m92Po53M05tmOe7fqSmR1ordkuM7uqm3Nsh5ltNbOHzWyPme02s5ta12e9ZgtsV9ZrZmb9ZvYjM/txa7v+rHV90np17Zi4mZU13VDiCkn7JT0h6Xp3f74rEyqQmb0mabu7Z12MYGaXSapK+id3v7B13V9KOubuX2698Q67+xe7Oc9U82zXlyRV3f0r3ZxbJ8xso6SN7r7TzIYkPSXpWkmfVcZrtsB2/YYyXjMzM0mD7l41sx5Jj0q6SdKvKGG9urknfqmkl9z9FXefknSPpGu6OB/MMldDEE2v0V2t3+/S9IspK/NsV/bc/aC772z9Pippj6TNynzNFtiurPm0autiT+ufK3G9upnEN0t6fcbl/ToNFqbFJT1kZk+Z2Y3dnkzBznT3g9L0i0vShi7Pp0ifa/WNvTO3Qw6zmdk5ki6W9LhOozWbtV1S5mtmZmUz2yXpsKQd7p68Xt1M4jbHdafL9x0/5u6XSPq0pN9v/fmO97avSTpf0kckHZT0192dTvvMbKWk+yTd7O4nuj2fosyxXdmvmbs33P0jmm5xeamZXZg6RjeT+H5JW2dc3iLpjS7NpVDu/kbr52FJ39H0oaPTxaHWMcp3jlUe7vJ8CuHuh1ovqKakf1Cma9Y6tnqfpLvd/f7W1dmv2VzbdbqsmSS5+3FJj2i6KX3SenUziT8haZuZnWtmvZKuk/RgF+dTCDMbbH34IjMblPQLkp5b+H9l5UFJN7R+v0HSA12cS2HeedG0/LIyXLPWB2V3SNrj7rfPuCnrNZtvu3JfMzNbb2ZrWr8PSPqUpL1KXK+uVmy2vhL0VUllSXe6+190bTIFMbPzNL33LU2fr/1fct2uVkOQT2j69JiHJN0m6V8lfVvS+yTtk/Tr7p7Vh4TzbNcnNP1nuUt6TdLvvXNcMhdm9nOSfijpWUnN1tV/rOnjx9mu2QLbdb0yXjMzu0jTH1yWNb1D/W13/3MzO0MJ60XZPQBkjIpNAMgYSRwAMkYSB4CMkcQBIGMkcQDIGEkcADJGEgeAjP0P1bpzH44QOc4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(features.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_source(fn, pad_to=max(lengths)):\n",
    "    ary = fn2features[fn]\n",
    "    example = np.zeros((pad_to, 13))\n",
    "    example[-ary.shape[0]:, :] = ary # padding from the left\n",
    "    return example.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_target(fns, pad_to=max(lengths)):\n",
    "    features = []\n",
    "    for fn in fns:\n",
    "        ary = fn2features[fn]\n",
    "        ary_zeros = np.zeros((pad_to, 13))\n",
    "        ary_zeros[:ary.shape[0], :] = ary\n",
    "        features.append(ary_zeros)\n",
    "    return np.concatenate(features, 1).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = Datasets(\n",
    "    df,\n",
    "    [lambda row: prepare_features_source(row.source_fn),\n",
    "     lambda row: prepare_features_target(row.target_fns),\n",
    "     lambda row: prepare_features_target(row.target_fns)],\n",
    "    n_inp=2,\n",
    "    splits = [train_examples.index, valid_examples.index]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 2048\n",
    "LR = 1e-3\n",
    "NUM_WORKERS = 10\n",
    "\n",
    "train_dl = DataLoader(dss.train, BS, NUM_WORKERS, shuffle=True)\n",
    "valid_dl = DataLoader(dss.valid, BS, NUM_WORKERS)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got the following error while training:\n",
    "\n",
    "# DataLoader worker (pid 2073) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.\n",
    "# trying the solution I found here: https://github.com/pytorch/pytorch/issues/5040\n",
    "# which is to execute\n",
    "\n",
    "!sudo umount /dev/shm/ && sudo mount -t tmpfs -o rw,nosuid,nodev,noexec,relatime,size=75G shm /dev/shm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0\n",
    "\n",
    "class Model(Module):\n",
    "    def __init__(self, hidden_size=50):\n",
    "        self.encoder= nn.LSTM(\n",
    "            input_size=13,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=4*13+2*hidden_size,\n",
    "            hidden_size=2*hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.lin = nn.Linear(2*hidden_size, 4*13)\n",
    "            \n",
    "    def forward(self, source_features, target_features):\n",
    "        _, (embeddings_dec, _) = self.encoder(source_features)\n",
    "#         embeddings_bi = torch.cat((embeddings_dec, embeddings_dec))\n",
    "#         embeddings_dec = embeddings_dec.permute(1,0,2)\n",
    "        embeddings_dec = embeddings_dec.view(embeddings_dec.shape[1], -1)\n",
    "        \n",
    "        outputs = torch.zeros_like(target_features)\n",
    "        input = target_features[:, :1, :]\n",
    "        outputs[:, 0, :] = input.squeeze()\n",
    "    \n",
    "        hidden = embeddings_dec.unsqueeze(0)\n",
    "        cell = torch.zeros_like(embeddings_dec).unsqueeze(0)\n",
    "        for t in range(1, target_features.shape[1]):\n",
    "            input = torch.cat((input, embeddings_dec.unsqueeze(1)), 2)\n",
    "            x, (hidden, cell) = self.decoder(input, (hidden, cell))\n",
    "            input = self.lin(x)\n",
    "            outputs[:, t, :] = input.squeeze()\n",
    "            \n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                input = target_features[:, t, :].unsqueeze(1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dl: break\n",
    "# model = Model().cuda()\n",
    "# out = model(batch[0].cuda(), batch[1].cuda())\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse_loss = MSELoss()\n",
    "# def modified_MSE(preds, targs):\n",
    "#     mask = targs == 0\n",
    "#     preds[mask] = 0\n",
    "#     return mse_loss(preds, targs)\n",
    "\n",
    "learn = Learner(dls.cuda(), Model().cuda(), loss_func=MSELoss(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>21.267969</td>\n",
       "      <td>21.719009</td>\n",
       "      <td>18:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>20.549919</td>\n",
       "      <td>21.043291</td>\n",
       "      <td>18:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20.231981</td>\n",
       "      <td>20.744255</td>\n",
       "      <td>18:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>20.119682</td>\n",
       "      <td>20.591694</td>\n",
       "      <td>18:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(4, cbs=SaveModelCallback(fname='4epochs_1e-3_adam', every_epoch=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate embedding for each unique word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_utterances = df.drop_duplicates(['source_fn'])\n",
    "df_unique_utterances.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss_all_utterances = Datasets(\n",
    "    df_unique_utterances,\n",
    "    [lambda row: prepare_features_source(row.source_fn), lambda row: 0],\n",
    "    n_inp=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dl = DataLoader(dss_all_utterances, BS, NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.9 s, sys: 18.2 s, total: 1min 15s\n",
      "Wall time: 5min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "all_embeddings = []\n",
    "with torch.no_grad():\n",
    "    learn.model.train = False\n",
    "    for batch in all_dl:\n",
    "        _, (embeddings, _) = learn.model.encoder(batch[0].cuda())\n",
    "        all_embeddings.append(embeddings.view(embeddings.shape[1], -1).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = torch.cat(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4140463, 100])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 13s, sys: 1.94 s, total: 6min 15s\n",
      "Wall time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "word2row_idxs = defaultdict(lambda: list())\n",
    "\n",
    "for idx, row in df_unique_utterances.iterrows():\n",
    "    word2row_idxs[row.source_word].append(idx)\n",
    "    \n",
    "word2embedding = {}\n",
    "\n",
    "for k, v in word2row_idxs.items():\n",
    "    word2embedding[k] = all_embeddings[np.array(v)].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered rows with nan values: 1\n"
     ]
    }
   ],
   "source": [
    "word2embedding_without_nans= {}\n",
    "nans_encountered = 0\n",
    "for k, v in word2embedding.items():\n",
    "    if k == k and (not np.isnan(v.numpy()).any()):\n",
    "        word2embedding_without_nans[k] = v.numpy()\n",
    "    else: nans_encountered += 1\n",
    "\n",
    "print(f'Encountered rows with nan values: {nans_encountered}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating embeddings using [word-embeddings-benchmarks](https://github.com/kudkudak/word-embeddings-benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from six import iteritems\n",
    "from web.datasets.similarity import fetch_MEN, fetch_WS353, fetch_SimLex999\n",
    "from web.embeddings import fetch_GloVe\n",
    "from web.evaluate import evaluate_similarity\n",
    "from web.embedding import Embedding, Vocabulary\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {\n",
    "    \"MEN\": fetch_MEN(),\n",
    "    \"WS353\": fetch_WS353(),\n",
    "    \"SIMLEX999\": fetch_SimLex999()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_embeddings = Embedding(\n",
    "    Vocabulary([w.lower() for w in list(word2embedding_without_nans.keys())]),\n",
    "    np.array(list(word2embedding_without_nans.values()))\n",
    ")\n",
    "\n",
    "speech2vec = KeyedVectors.load_word2vec_format('../speech2vec-pretrained-vectors/speech2vec/50.vec', binary=False) \n",
    "speech2vec_embeddings = Embedding(Vocabulary(list(speech2vec.vocab.keys())), speech2vec.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing 392 words. Will replace them with mean vector\n",
      "/opt/conda/lib/python3.7/site-packages/web-0.0.1-py3.7.egg/web/evaluate.py:336: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  A = np.vstack(w.get(word, mean_vector) for word in X[:, 0])\n",
      "/opt/conda/lib/python3.7/site-packages/web-0.0.1-py3.7.egg/web/evaluate.py:337: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  B = np.vstack(w.get(word, mean_vector) for word in X[:, 1])\n",
      "Missing 61 words. Will replace them with mean vector\n",
      "Missing 24 words. Will replace them with mean vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation of scores on MEN 0.5896756323911225\n",
      "Spearman correlation of scores on WS353 0.49890235673392536\n",
      "Spearman correlation of scores on SIMLEX999 0.28202624769092116\n"
     ]
    }
   ],
   "source": [
    "for name, data in iteritems(tasks):\n",
    "    print(\"Spearman correlation of scores on {} {}\".format(name, evaluate_similarity(speech2vec_embeddings, data.X, data.y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing 272 words. Will replace them with mean vector\n",
      "Missing 50 words. Will replace them with mean vector\n",
      "Missing 13 words. Will replace them with mean vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation of scores on MEN 0.046339262520846226\n",
      "Spearman correlation of scores on WS353 -0.02145477424849065\n",
      "Spearman correlation of scores on SIMLEX999 -0.13253222139431725\n"
     ]
    }
   ],
   "source": [
    "for name, data in iteritems(tasks):\n",
    "    print(\"Spearman correlation of scores on {} {}\".format(name, evaluate_similarity(our_embeddings, data.X, data.y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
