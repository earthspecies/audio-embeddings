{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.learner import *\n",
    "from fastai.data.all import *\n",
    "from fastai.callback.tracker import SaveModelCallback\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib2 import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.nn import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.2 s, sys: 2.63 s, total: 26.8 s\n",
      "Wall time: 28.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17937758, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv('data/examples.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While attempting to train the model, it turned out that the reading and unpickling operation done at this scale, with so many files, is very computationally expensive.\n",
    "\n",
    "But the examples in the mfcc represenatations are very small. Let's read them all into the memory before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniq_fns = np.unique(df.target_fn.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# fn2feature = {}\n",
    "# for fn in uniq_fns:\n",
    "#     ary = pd.read_pickle(f'data/examples/{fn}.pkl')\n",
    "#     fn2feature[fn] = ary\n",
    "\n",
    "# pd.to_pickle(fn2feature, 'data/fn2feature.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.3 s, sys: 14.2 s, total: 31.4 s\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fn2features = pd.read_pickle('data/fn2feature.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(df.source_word.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# counter = Counter(df.source_word)\n",
    "\n",
    "# most_common_words = set([t[0] for t in counter.most_common(1000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suspect that reading the data from a file saved within numpy (`numpy.save`) is much less computationally expensive than unpickling it, but I might be wrong. Either way, at ~4 million of unique utterances, the dataset is small enough to comfortably fit within memory of a GCP instance (at ~53GBs used RAM during training).\n",
    "\n",
    "This might not be ideal for experimentation on home rigs. Saving the data using `numpy.save` and evaluating performance would definitely be a very interesting and useful exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_word</th>\n",
       "      <th>target_word</th>\n",
       "      <th>source_fn</th>\n",
       "      <th>target_fn</th>\n",
       "      <th>set_name</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>distance_from_target</th>\n",
       "      <th>audio_fpath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>FELT</td>\n",
       "      <td>8af4aebcf4a74004b02db9f88d99e89a</td>\n",
       "      <td>1cb9442ec1a6468282da309756e2ff57</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>1</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>THAT</td>\n",
       "      <td>8af4aebcf4a74004b02db9f88d99e89a</td>\n",
       "      <td>2f60546c930c47068ee0a129e6d51c39</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>2</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FELT</td>\n",
       "      <td>I</td>\n",
       "      <td>1cb9442ec1a6468282da309756e2ff57</td>\n",
       "      <td>8af4aebcf4a74004b02db9f88d99e89a</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>1</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FELT</td>\n",
       "      <td>THAT</td>\n",
       "      <td>1cb9442ec1a6468282da309756e2ff57</td>\n",
       "      <td>2f60546c930c47068ee0a129e6d51c39</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>1</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FELT</td>\n",
       "      <td>IT</td>\n",
       "      <td>1cb9442ec1a6468282da309756e2ff57</td>\n",
       "      <td>155ad336d88c4cbf814a1237983b5b18</td>\n",
       "      <td>train-clean-360</td>\n",
       "      <td>7000</td>\n",
       "      <td>83696</td>\n",
       "      <td>2</td>\n",
       "      <td>data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source_word target_word                         source_fn  \\\n",
       "0           I        FELT  8af4aebcf4a74004b02db9f88d99e89a   \n",
       "1           I        THAT  8af4aebcf4a74004b02db9f88d99e89a   \n",
       "2        FELT           I  1cb9442ec1a6468282da309756e2ff57   \n",
       "3        FELT        THAT  1cb9442ec1a6468282da309756e2ff57   \n",
       "4        FELT          IT  1cb9442ec1a6468282da309756e2ff57   \n",
       "\n",
       "                          target_fn         set_name  speaker_id  book_id  \\\n",
       "0  1cb9442ec1a6468282da309756e2ff57  train-clean-360        7000    83696   \n",
       "1  2f60546c930c47068ee0a129e6d51c39  train-clean-360        7000    83696   \n",
       "2  8af4aebcf4a74004b02db9f88d99e89a  train-clean-360        7000    83696   \n",
       "3  2f60546c930c47068ee0a129e6d51c39  train-clean-360        7000    83696   \n",
       "4  155ad336d88c4cbf814a1237983b5b18  train-clean-360        7000    83696   \n",
       "\n",
       "   distance_from_target  \\\n",
       "0                     1   \n",
       "1                     2   \n",
       "2                     1   \n",
       "3                     1   \n",
       "4                     2   \n",
       "\n",
       "                                                        audio_fpath  \n",
       "0  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  \n",
       "1  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  \n",
       "2  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  \n",
       "3  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  \n",
       "4  data/LibriSpeech/train-clean-360/7000/83696/7000-83696-0000.flac  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = df[df.set_name.isin(['train-clean-360', 'train-clean-100', 'dev-clean'])]\n",
    "valid_examples = df[df.set_name == 'test-clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159688530, 1751292)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples.size, valid_examples.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.9 s, sys: 144 ms, total: 5.05 s\n",
      "Wall time: 5.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "unique_fns = df.source_fn.unique()\n",
    "np.random.shuffle(unique_fns)\n",
    "lengths = []\n",
    "for i, features in enumerate(fn2features.values()):\n",
    "    lengths.append(features.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.028019713968394"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc6f952f350>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(features.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mean = -5\n",
    "dataset_std = 15\n",
    "\n",
    "def normalize_data(ary):\n",
    "    return (ary - dataset_mean) / dataset_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 12s, sys: 1.97 s, total: 22min 14s\n",
      "Wall time: 22min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "word2row_idxs = defaultdict(lambda: list())\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    word2row_idxs[row.source_word].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(word, fns, pad_to=max(lengths), pad_left=False):\n",
    "    row_idx = np.random.randint(len(word2row_idxs[word]))\n",
    "    ary = fn2features[fns[row_idx]]\n",
    "    example = np.zeros((pad_to, 13))\n",
    "    if pad_left:\n",
    "        example[-ary.shape[0]:, :] = ary\n",
    "    else: example[:ary.shape[0], :] = ary\n",
    "    return example.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = Datasets(\n",
    "    vocab * 5,\n",
    "    [lambda word: normalize_data(prepare_features(word, df.source_fn, pad_left=True)),\n",
    "     lambda word: normalize_data(prepare_features(word, df.target_fn)),\n",
    "     lambda word: normalize_data(prepare_features(word, df.target_fn))],\n",
    "    n_inp=2,\n",
    "    splits = [np.arange(len(vocab)*4), np.arange(len(vocab)*4, len(vocab)*5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 32\n",
    "LR = 1e-3\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "train_dl = DataLoader(dss.train, BS, NUM_WORKERS, shuffle=True)\n",
    "valid_dl = DataLoader(dss.valid, BS, NUM_WORKERS)\n",
    "\n",
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Got the following error while training:\n",
    "\n",
    "# DataLoader worker (pid 2073) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.\n",
    "# trying the solution I found here: https://github.com/pytorch/pytorch/issues/5040\n",
    "# which is to execute\n",
    "!sudo umount /dev/shm/ && sudo mount -t tmpfs -o rw,nosuid,nodev,noexec,relatime,size=50G shm /dev/shm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # unidirectional encoder\n",
    "# teacher_forcing_ratio = 0\n",
    "\n",
    "# class Model(Module):\n",
    "#     def __init__(self, hidden_size=50):\n",
    "#         self.encoder= nn.LSTM(\n",
    "#             input_size=13,\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_layers=1,\n",
    "#             batch_first=True,\n",
    "#             dropout=0,\n",
    "#             bidirectional=False\n",
    "#         )\n",
    "#         self.decoder = nn.LSTM(\n",
    "#             input_size=hidden_size+13,\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_layers=1,\n",
    "#             batch_first=True,\n",
    "#             dropout=0,\n",
    "#             bidirectional=False\n",
    "#         )\n",
    "#         self.lin = nn.Linear(hidden_size, 13)\n",
    "            \n",
    "#     def forward(self, source_features, target_features):\n",
    "#         _, (embeddings_dec, _) = self.encoder(source_features)\n",
    "        \n",
    "#         outputs = torch.zeros_like(target_features)\n",
    "#         input = target_features[:, 4:5, :]\n",
    "#         outputs[:, :4, :] = target_features[:, :4, :]\n",
    "        \n",
    "#         hidden = embeddings_dec\n",
    "#         embeddings_dec = embeddings_dec.squeeze(0)\n",
    "        \n",
    "#         cell = torch.zeros_like(embeddings_dec).unsqueeze(0)\n",
    "#         for t in range(1, target_features.shape[1]):\n",
    "#             input = torch.cat((input, embeddings_dec.unsqueeze(1)), 2)\n",
    "#             x, (hidden, cell) = self.decoder(input, (hidden, cell))\n",
    "#             x = self.lin(x)\n",
    "#             input = (torch.sigmoid(x) - 0.5) * 16\n",
    "#             outputs[:, t, :] = input.squeeze()\n",
    "            \n",
    "#             if random.random() < teacher_forcing_ratio:\n",
    "#                 input = target_features[:, t, :].unsqueeze(1)\n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher_forcing_ratio = 0\n",
    "\n",
    "# class Model(Module):\n",
    "#     def __init__(self, hidden_size=50):\n",
    "#         self.encoder= nn.LSTM(\n",
    "#             input_size=13,\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_layers=1,\n",
    "#             batch_first=True,\n",
    "#             dropout=0,\n",
    "#             bidirectional=False\n",
    "#         )\n",
    "#         self.decoder = nn.LSTM(\n",
    "#             input_size=hidden_size+13,\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_layers=1,\n",
    "#             batch_first=True,\n",
    "#             dropout=0,\n",
    "#             bidirectional=False\n",
    "#         )\n",
    "#         self.lin = nn.Linear(hidden_size, 13)\n",
    "            \n",
    "#     def forward(self, source_features, target_features):\n",
    "#         _, (embeddings, _) = self.encoder(source_features)\n",
    "#         input = torch.cat(\n",
    "#             (\n",
    "#                 target_features[:, :-1, :],\n",
    "#                 embeddings.permute(1, 0, 2).repeat(1, target_features.shape[1]-1, 1)\n",
    "#             ), 2)\n",
    "#         cell = torch.zeros_like(embeddings)\n",
    "#         x, _ = self.decoder(input, (embeddings, cell))\n",
    "#         return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unidirectional encoder\n",
    "teacher_forcing_ratio = 0\n",
    "\n",
    "class Model(Module):\n",
    "    def __init__(self, hidden_size=50):\n",
    "        self.encoder= nn.LSTM(\n",
    "            input_size=13,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=hidden_size+13,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.lin = nn.Linear(hidden_size, 13)\n",
    "            \n",
    "    def forward(self, source_features, target_features):\n",
    "        _, (embeddings_dec, _) = self.encoder(source_features)\n",
    "        \n",
    "        outputs = torch.zeros_like(target_features)\n",
    "        input = target_features[:, :1, :]\n",
    "        outputs[:, 0, :] = input.squeeze()\n",
    "        \n",
    "        hidden = embeddings_dec\n",
    "        embeddings_dec = embeddings_dec.squeeze(0)\n",
    "        \n",
    "        cell = torch.zeros_like(embeddings_dec).unsqueeze(0)\n",
    "        for t in range(1, target_features.shape[1]):\n",
    "            input = torch.cat((input, embeddings_dec.unsqueeze(1)), 2)\n",
    "            x, (hidden, cell) = self.decoder(input, (hidden, cell))\n",
    "            x = self.lin(x)\n",
    "            input = (torch.sigmoid(x) - 0.5) * 16\n",
    "            outputs[:, t, :] = input.squeeze()\n",
    "            \n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                input = target_features[:, t, :].unsqueeze(1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bidirectional encoder\n",
    "#\n",
    "# teacher_forcing_ratio = 0\n",
    "\n",
    "# class Model(Module):\n",
    "#     def __init__(self, hidden_size=50):\n",
    "#         self.encoder= nn.LSTM(\n",
    "#             input_size=13,\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_layers=1,\n",
    "#             batch_first=True,\n",
    "#             dropout=0,\n",
    "#             bidirectional=True\n",
    "#         )\n",
    "#         self.decoder = nn.LSTM(\n",
    "#             input_size=2*hidden_size+13,\n",
    "#             hidden_size=2*hidden_size,\n",
    "#             num_layers=1,\n",
    "#             batch_first=True,\n",
    "#             dropout=0,\n",
    "#             bidirectional=False\n",
    "#         )\n",
    "#         self.lin = nn.Linear(2*hidden_size, 13)\n",
    "            \n",
    "#     def forward(self, source_features, target_features):\n",
    "#         _, (embeddings_dec, _) = self.encoder(source_features)\n",
    "#         embeddings_dec = embeddings_dec.view(embeddings_dec.shape[1], -1)\n",
    "        \n",
    "#         outputs = torch.zeros_like(target_features)\n",
    "#         input = target_features[:, :1, :]\n",
    "#         outputs[:, 0, :] = input.squeeze()\n",
    "    \n",
    "#         hidden = embeddings_dec.unsqueeze(0)\n",
    "#         cell = torch.zeros_like(embeddings_dec).unsqueeze(0)\n",
    "#         for t in range(1, target_features.shape[1]):\n",
    "#             input = torch.cat((input, embeddings_dec.unsqueeze(1)), 2)\n",
    "#             x, (hidden, cell) = self.decoder(input, (hidden, cell))\n",
    "#             x = self.lin(x)\n",
    "#             input = (torch.sigmoid(x) - 0.5) * 16\n",
    "#             outputs[:, t, :] = input.squeeze()\n",
    "            \n",
    "#             if random.random() < teacher_forcing_ratio:\n",
    "#                 input = target_features[:, t, :].unsqueeze(1)\n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse_loss = MSELoss()\n",
    "# def modified_MSE(preds, targs):\n",
    "#     mask = targs == 0\n",
    "#     preds[mask] = 0\n",
    "#     return mse_loss(preds, targs)\n",
    "\n",
    "mse_loss = MSELoss()\n",
    "def targ_trunc_MSE(preds, targs):\n",
    "    return mse_loss(preds, targs[:, 1:, :])\n",
    "\n",
    "learn = Learner(dls.cuda(), Model().cuda(), loss_func=MSELoss(), lr=1e-3, opt_func=SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='18' class='' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      45.00% [18/40 17:44:08<21:40:37]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.078469</td>\n",
       "      <td>0.079252</td>\n",
       "      <td>54:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.078155</td>\n",
       "      <td>0.078031</td>\n",
       "      <td>54:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.076025</td>\n",
       "      <td>0.077604</td>\n",
       "      <td>56:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.075850</td>\n",
       "      <td>0.076503</td>\n",
       "      <td>59:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.074447</td>\n",
       "      <td>0.074504</td>\n",
       "      <td>55:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.069219</td>\n",
       "      <td>0.068355</td>\n",
       "      <td>55:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.065502</td>\n",
       "      <td>0.063539</td>\n",
       "      <td>55:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.061216</td>\n",
       "      <td>0.061305</td>\n",
       "      <td>57:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.059896</td>\n",
       "      <td>0.061053</td>\n",
       "      <td>58:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.061091</td>\n",
       "      <td>0.060678</td>\n",
       "      <td>58:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.061042</td>\n",
       "      <td>0.060431</td>\n",
       "      <td>58:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.059705</td>\n",
       "      <td>0.060362</td>\n",
       "      <td>58:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.060526</td>\n",
       "      <td>0.059944</td>\n",
       "      <td>1:02:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.060235</td>\n",
       "      <td>0.060345</td>\n",
       "      <td>1:03:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.060429</td>\n",
       "      <td>0.060020</td>\n",
       "      <td>1:03:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.059309</td>\n",
       "      <td>0.059841</td>\n",
       "      <td>1:03:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.060271</td>\n",
       "      <td>0.059731</td>\n",
       "      <td>1:03:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.061295</td>\n",
       "      <td>0.059643</td>\n",
       "      <td>1:03:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3114' class='' max='8333' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      37.37% [3114/8333 21:40<36:19 0.0599]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(40, cbs=SaveModelCallback(fname='1e-3_SGD', every_epoch=True), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate embedding for each unique word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_utterances = df.drop_duplicates(['source_fn'])\n",
    "df_unique_utterances.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss_all_utterances = Datasets(\n",
    "    df_unique_utterances,\n",
    "    [lambda row: normalize_data(prepare_features(row.source_fn, pad_left=True)),\n",
    "     lambda row: normalize_data(prepare_features(row.target_fn))],\n",
    "    n_inp=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dl = DataLoader(dss_all_utterances, BS, NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "all_embeddings = []\n",
    "with torch.no_grad():\n",
    "    learn.model.train = False\n",
    "    for batch in all_dl:\n",
    "        _, (embeddings, _) = learn.model.encoder(batch[0].cuda())\n",
    "        all_embeddings.append(embeddings.view(embeddings.shape[1], -1).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = torch.cat(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in all_dl:\n",
    "        outputs = learn.model(batch[0].cuda(), batch[1].cuda())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(outputs[-1].cpu().numpy().T[:, :20])\n",
    "axs[1].imshow(batch[1][-1].cpu().numpy().T[:, :20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(outputs[-10].cpu().numpy().T[:, :20])\n",
    "axs[1].imshow(batch[1][-10].cpu().numpy().T[:, :20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(outputs[0].cpu().numpy().T[:, :20])\n",
    "axs[1].imshow(batch[1][0].cpu().numpy().T[:, :20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "word2row_idxs = defaultdict(lambda: list())\n",
    "\n",
    "for idx, row in df_unique_utterances.iterrows():\n",
    "    word2row_idxs[row.source_word].append(idx)\n",
    "    \n",
    "word2embedding = {}\n",
    "\n",
    "for k, v in word2row_idxs.items():\n",
    "    word2embedding[k] = all_embeddings[np.array(v)].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embedding_without_nans= {}\n",
    "nans_encountered = 0\n",
    "for k, v in word2embedding.items():\n",
    "    if k == k and (not np.isnan(v.numpy()).any()):\n",
    "        word2embedding_without_nans[k] = v.numpy()\n",
    "    else: nans_encountered += 1\n",
    "\n",
    "print(f'Encountered rows with nan values: {nans_encountered}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating embeddings using [word-embeddings-benchmarks](https://github.com/kudkudak/word-embeddings-benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from six import iteritems\n",
    "from web.datasets.similarity import fetch_MEN, fetch_WS353, fetch_SimLex999\n",
    "from web.embeddings import fetch_GloVe\n",
    "from web.evaluate import evaluate_similarity\n",
    "from web.embedding import Embedding, Vocabulary\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {\n",
    "    \"MEN\": fetch_MEN(),\n",
    "    \"WS353\": fetch_WS353(),\n",
    "    \"SIMLEX999\": fetch_SimLex999()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_embeddings = Embedding(\n",
    "    Vocabulary([w.lower() for w in list(word2embedding_without_nans.keys())]),\n",
    "    np.array(list(word2embedding_without_nans.values()))\n",
    ")\n",
    "\n",
    "speech2vec = KeyedVectors.load_word2vec_format('../speech2vec-pretrained-vectors/speech2vec/50.vec', binary=False) \n",
    "speech2vec_embeddings = Embedding(Vocabulary(list(speech2vec.vocab.keys())), speech2vec.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in iteritems(tasks):\n",
    "    print(\"Spearman correlation of scores on {} {}\".format(name, evaluate_similarity(speech2vec_embeddings, data.X, data.y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in iteritems(tasks):\n",
    "    print(\"Spearman correlation of scores on {} {}\".format(name, evaluate_similarity(our_embeddings, data.X, data.y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
